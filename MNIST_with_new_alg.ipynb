{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-1xNKXx6TBQp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import new_alg_v2 as na\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "# MNIST dataset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "    \n",
        "def define_dataloaders(n_examples_train, n_examples_test, classes=np.arange(10), zscore_images=True):\n",
        "    # MNIST data, batch training\n",
        "    batch_size = n_examples_train\n",
        "    batches_per_epoch_train = n_examples_train / batch_size\n",
        "    batches_per_epoch_test = n_examples_test / batch_size\n",
        "\n",
        "    # Choose the classes (at most 10)\n",
        "    assert max(classes) <= 9\n",
        "\n",
        "    # Transformation for the images\n",
        "    transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "    #transform = transforms.Compose([transforms.ToTensor(),\n",
        "    #                              transforms.Normalize((0.5,), (0.5,)),\n",
        "    #                              ])\n",
        "    trainset = MNIST(data_dir, download=True, train=True, transform=transform)\n",
        "    testset = MNIST(data_dir, download=True, train=False, transform=transform)\n",
        "\n",
        "    # Obtain training and test data. \n",
        "    # Note that both datasets are sorted, but the train and test loaders will shuffle them during training.\n",
        "    n_examples_tt = [n_examples_train, n_examples_test]\n",
        "    for i_d, (n_examples_i, dataset) in enumerate(zip(n_examples_tt, [trainset, testset])):\n",
        "        n_per_class = n_examples_i // len(classes)\n",
        "        data_orig = dataset.data.detach().clone()\n",
        "        targets_orig = dataset.targets.detach().clone()\n",
        "        for i_c, class_i in enumerate(classes):\n",
        "            mask = targets_orig == class_i\n",
        "            i0 = i_c * n_per_class\n",
        "            i1 = (i_c+1) * n_per_class\n",
        "            dataset.data[i0:i1] = data_orig[mask][:n_per_class]\n",
        "            dataset.targets[i0:i1] = targets_orig[mask][:n_per_class]\n",
        "        # Fill the remaining slots with random classes from the available choices\n",
        "        n_remain = n_examples_i - i1 \n",
        "        for i in range(n_remain):\n",
        "            class_i = np.random.choice(classes)\n",
        "            mask = targets_orig == class_i\n",
        "            idx_i = np.random.choice(torch.where(mask)[0][i1:].cpu())\n",
        "            dataset.data[i1+i] = data_orig[idx_i]\n",
        "            dataset.targets[i1+i] = targets_orig[idx_i]\n",
        "\n",
        "        # Cut off\n",
        "        dataset.data = dataset.data[:n_examples_i]\n",
        "        dataset.targets = dataset.targets[:n_examples_i]\n",
        "\n",
        "    # Batch-loader\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rRSXyp6THwR",
        "outputId": "edd158a8-0f57-48d7-be07-55ffef562fac"
      },
      "outputs": [],
      "source": [
        "n_epochs = 5000\n",
        "learning_rate = 0.01\n",
        "momentum = 0\n",
        "torch.backends.cudnn.enabled = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_HKx24x7V_4d"
      },
      "outputs": [],
      "source": [
        "## no dopout\n",
        "#Building the Network\n",
        "\n",
        "conv1_out= 10\n",
        "conv2_out=20\n",
        "fc1_in=320\n",
        "fc2_in=50\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=5)\n",
        "        #self.conv2_drop = nn.Dropout2d() --- no dropout\n",
        "        self.fc1 = nn.Linear(fc1_in, fc2_in)\n",
        "        self.fc2 = nn.Linear(fc2_in, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(F.max_pool2d(self.conv1(x), 2))\n",
        "        #x = torch.tanh(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = torch.tanh(F.max_pool2d((self.conv2(x)), 2))\n",
        "        x = x.view(-1, fc1_in)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "      #  x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "taugfbRTWCo-"
      },
      "outputs": [],
      "source": [
        "network = Net()\n",
        "# with new alg\n",
        "optimizerSGD = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
        "optimizer = na.new_alg(network.parameters(), lr=learning_rate, momentum=momentum)\n",
        "loss_f=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the initial weights to measure later their contribution:\n",
        "fc1_init = network.fc1.weight.clone()\n",
        "fc2_init = network.fc2.weight.clone()\n",
        "conv1_init = network.conv1.weight.clone()\n",
        "conv2_init = network.conv2.weight.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XKYJb4e7WLNG"
      },
      "outputs": [],
      "source": [
        "data_dir = '/files/'\n",
        "n_examples_train = 1000\n",
        "n_examples_test = 500\n",
        "train_loader, test_loader = define_dataloaders(n_examples_train, n_examples_test)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y2MhJ3U3WNTW"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "  train_n = 5\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizerSGD.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = loss_f(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % train_n == 0:\n",
        "      print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
        "          epoch,\n",
        "          loss.item()))\n",
        "    train_losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r9W0MBhnWPfg"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += loss_f(output,target)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= 1 #1 is the amount of test batches\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss,\n",
        "    correct,\n",
        "    len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "BJo6gYxIWSUC",
        "outputId": "9cd112a3-872f-4d2c-c351-b99e8929a6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3163, Accuracy: 38/500 (8%)\n",
            "\n",
            "Train Epoch: 5 \tLoss: 2.278673\n",
            "Train Epoch: 10 \tLoss: 2.245437\n",
            "Train Epoch: 15 \tLoss: 2.162067\n",
            "Train Epoch: 20 \tLoss: 2.110683\n",
            "Train Epoch: 25 \tLoss: 2.014198\n",
            "\n",
            "Test set: Avg. loss: 2.0057, Accuracy: 262/500 (52%)\n",
            "\n",
            "Train Epoch: 30 \tLoss: 1.889400\n",
            "Train Epoch: 35 \tLoss: 1.742770\n",
            "Train Epoch: 40 \tLoss: 1.621186\n",
            "Train Epoch: 45 \tLoss: 1.507872\n",
            "Train Epoch: 50 \tLoss: 1.463808\n",
            "\n",
            "Test set: Avg. loss: 1.4719, Accuracy: 347/500 (69%)\n",
            "\n",
            "Train Epoch: 55 \tLoss: 1.279770\n",
            "Train Epoch: 60 \tLoss: 1.185118\n",
            "Train Epoch: 65 \tLoss: 1.103558\n",
            "Train Epoch: 70 \tLoss: 1.004975\n",
            "Train Epoch: 75 \tLoss: 0.974808\n",
            "\n",
            "Test set: Avg. loss: 1.0555, Accuracy: 366/500 (73%)\n",
            "\n",
            "Train Epoch: 80 \tLoss: 0.884220\n",
            "Train Epoch: 85 \tLoss: 0.859074\n",
            "Train Epoch: 90 \tLoss: 0.780369\n",
            "Train Epoch: 95 \tLoss: 0.726597\n",
            "Train Epoch: 100 \tLoss: 0.685782\n",
            "\n",
            "Test set: Avg. loss: 0.8390, Accuracy: 398/500 (80%)\n",
            "\n",
            "Train Epoch: 105 \tLoss: 0.663117\n",
            "Train Epoch: 110 \tLoss: 0.636805\n",
            "Train Epoch: 115 \tLoss: 0.621631\n",
            "Train Epoch: 120 \tLoss: 0.570213\n",
            "Train Epoch: 125 \tLoss: 0.538893\n",
            "\n",
            "Test set: Avg. loss: 0.6935, Accuracy: 419/500 (84%)\n",
            "\n",
            "Train Epoch: 130 \tLoss: 0.515273\n",
            "Train Epoch: 135 \tLoss: 0.499116\n",
            "Train Epoch: 140 \tLoss: 0.479047\n",
            "Train Epoch: 145 \tLoss: 0.465689\n",
            "Train Epoch: 150 \tLoss: 0.441188\n",
            "\n",
            "Test set: Avg. loss: 0.6188, Accuracy: 423/500 (85%)\n",
            "\n",
            "Train Epoch: 155 \tLoss: 0.441842\n",
            "Train Epoch: 160 \tLoss: 0.422822\n",
            "Train Epoch: 165 \tLoss: 0.409737\n",
            "Train Epoch: 170 \tLoss: 0.424669\n",
            "Train Epoch: 175 \tLoss: 0.371597\n",
            "\n",
            "Test set: Avg. loss: 0.5331, Accuracy: 428/500 (86%)\n",
            "\n",
            "Train Epoch: 180 \tLoss: 0.389706\n",
            "Train Epoch: 185 \tLoss: 0.355325\n",
            "Train Epoch: 190 \tLoss: 0.340810\n",
            "Train Epoch: 195 \tLoss: 0.329379\n",
            "Train Epoch: 200 \tLoss: 0.319987\n",
            "\n",
            "Test set: Avg. loss: 0.4817, Accuracy: 432/500 (86%)\n",
            "\n",
            "Train Epoch: 205 \tLoss: 0.310003\n",
            "Train Epoch: 210 \tLoss: 0.301279\n",
            "Train Epoch: 215 \tLoss: 0.293914\n",
            "Train Epoch: 220 \tLoss: 0.293875\n",
            "Train Epoch: 225 \tLoss: 0.278682\n",
            "\n",
            "Test set: Avg. loss: 0.4421, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 230 \tLoss: 0.270259\n",
            "Train Epoch: 235 \tLoss: 0.263415\n",
            "Train Epoch: 240 \tLoss: 0.263344\n",
            "Train Epoch: 245 \tLoss: 0.251435\n",
            "Train Epoch: 250 \tLoss: 0.251020\n",
            "\n",
            "Test set: Avg. loss: 0.4174, Accuracy: 439/500 (88%)\n",
            "\n",
            "Train Epoch: 255 \tLoss: 0.238856\n",
            "Train Epoch: 260 \tLoss: 0.234376\n",
            "Train Epoch: 265 \tLoss: 0.228439\n",
            "Train Epoch: 270 \tLoss: 0.223587\n",
            "Train Epoch: 275 \tLoss: 0.225238\n",
            "\n",
            "Test set: Avg. loss: 0.3956, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 280 \tLoss: 0.215103\n",
            "Train Epoch: 285 \tLoss: 0.209233\n",
            "Train Epoch: 290 \tLoss: 0.208007\n",
            "Train Epoch: 295 \tLoss: 0.201528\n",
            "Train Epoch: 300 \tLoss: 0.196182\n",
            "\n",
            "Test set: Avg. loss: 0.3736, Accuracy: 439/500 (88%)\n",
            "\n",
            "Train Epoch: 305 \tLoss: 0.192858\n",
            "Train Epoch: 310 \tLoss: 0.190808\n",
            "Train Epoch: 315 \tLoss: 0.185123\n",
            "Train Epoch: 320 \tLoss: 0.182241\n",
            "Train Epoch: 325 \tLoss: 0.177959\n",
            "\n",
            "Test set: Avg. loss: 0.3539, Accuracy: 443/500 (89%)\n",
            "\n",
            "Train Epoch: 330 \tLoss: 0.174379\n",
            "Train Epoch: 335 \tLoss: 0.171631\n",
            "Train Epoch: 340 \tLoss: 0.168305\n",
            "Train Epoch: 345 \tLoss: 0.165807\n",
            "Train Epoch: 350 \tLoss: 0.162463\n",
            "\n",
            "Test set: Avg. loss: 0.3409, Accuracy: 445/500 (89%)\n",
            "\n",
            "Train Epoch: 355 \tLoss: 0.161607\n",
            "Train Epoch: 360 \tLoss: 0.156024\n",
            "Train Epoch: 365 \tLoss: 0.152907\n",
            "Train Epoch: 370 \tLoss: 0.150717\n",
            "Train Epoch: 375 \tLoss: 0.149390\n",
            "\n",
            "Test set: Avg. loss: 0.3272, Accuracy: 447/500 (89%)\n",
            "\n",
            "Train Epoch: 380 \tLoss: 0.145178\n",
            "Train Epoch: 385 \tLoss: 0.145745\n",
            "Train Epoch: 390 \tLoss: 0.140507\n",
            "Train Epoch: 395 \tLoss: 0.138457\n",
            "Train Epoch: 400 \tLoss: 0.136480\n",
            "\n",
            "Test set: Avg. loss: 0.3167, Accuracy: 448/500 (90%)\n",
            "\n",
            "Train Epoch: 405 \tLoss: 0.133415\n",
            "Train Epoch: 410 \tLoss: 0.132644\n",
            "Train Epoch: 415 \tLoss: 0.129112\n",
            "Train Epoch: 420 \tLoss: 0.127236\n",
            "Train Epoch: 425 \tLoss: 0.127137\n",
            "\n",
            "Test set: Avg. loss: 0.3096, Accuracy: 447/500 (89%)\n",
            "\n",
            "Train Epoch: 430 \tLoss: 0.123044\n",
            "Train Epoch: 435 \tLoss: 0.121210\n",
            "Train Epoch: 440 \tLoss: 0.120358\n",
            "Train Epoch: 445 \tLoss: 0.117252\n",
            "Train Epoch: 450 \tLoss: 0.116685\n",
            "\n",
            "Test set: Avg. loss: 0.3010, Accuracy: 452/500 (90%)\n",
            "\n",
            "Train Epoch: 455 \tLoss: 0.113966\n",
            "Train Epoch: 460 \tLoss: 0.111932\n",
            "Train Epoch: 465 \tLoss: 0.110439\n",
            "Train Epoch: 470 \tLoss: 0.109077\n",
            "Train Epoch: 475 \tLoss: 0.107096\n",
            "\n",
            "Test set: Avg. loss: 0.2915, Accuracy: 453/500 (91%)\n",
            "\n",
            "Train Epoch: 480 \tLoss: 0.105549\n",
            "Train Epoch: 485 \tLoss: 0.104097\n",
            "Train Epoch: 490 \tLoss: 0.102705\n",
            "Train Epoch: 495 \tLoss: 0.103001\n",
            "Train Epoch: 500 \tLoss: 0.100045\n",
            "\n",
            "Test set: Avg. loss: 0.2836, Accuracy: 453/500 (91%)\n",
            "\n",
            "Train Epoch: 505 \tLoss: 0.098288\n",
            "Train Epoch: 510 \tLoss: 0.097170\n",
            "Train Epoch: 515 \tLoss: 0.095158\n",
            "Train Epoch: 520 \tLoss: 0.093912\n",
            "Train Epoch: 525 \tLoss: 0.093533\n",
            "\n",
            "Test set: Avg. loss: 0.2749, Accuracy: 453/500 (91%)\n",
            "\n",
            "Train Epoch: 530 \tLoss: 0.091469\n",
            "Train Epoch: 535 \tLoss: 0.090075\n",
            "Train Epoch: 540 \tLoss: 0.089128\n",
            "Train Epoch: 545 \tLoss: 0.088252\n",
            "Train Epoch: 550 \tLoss: 0.086791\n",
            "\n",
            "Test set: Avg. loss: 0.2755, Accuracy: 453/500 (91%)\n",
            "\n",
            "Train Epoch: 555 \tLoss: 0.085215\n",
            "Train Epoch: 560 \tLoss: 0.084970\n",
            "Train Epoch: 565 \tLoss: 0.082911\n",
            "Train Epoch: 570 \tLoss: 0.081765\n",
            "Train Epoch: 575 \tLoss: 0.080835\n",
            "\n",
            "Test set: Avg. loss: 0.2672, Accuracy: 454/500 (91%)\n",
            "\n",
            "Train Epoch: 580 \tLoss: 0.080532\n",
            "Train Epoch: 585 \tLoss: 0.078788\n",
            "Train Epoch: 590 \tLoss: 0.077751\n",
            "Train Epoch: 595 \tLoss: 0.076831\n",
            "Train Epoch: 600 \tLoss: 0.076230\n",
            "\n",
            "Test set: Avg. loss: 0.2614, Accuracy: 457/500 (91%)\n",
            "\n",
            "Train Epoch: 605 \tLoss: 0.074854\n",
            "Train Epoch: 610 \tLoss: 0.073940\n",
            "Train Epoch: 615 \tLoss: 0.073233\n",
            "Train Epoch: 620 \tLoss: 0.072137\n",
            "Train Epoch: 625 \tLoss: 0.071835\n",
            "\n",
            "Test set: Avg. loss: 0.2592, Accuracy: 457/500 (91%)\n",
            "\n",
            "Train Epoch: 630 \tLoss: 0.070386\n",
            "Train Epoch: 635 \tLoss: 0.069501\n",
            "Train Epoch: 640 \tLoss: 0.068538\n",
            "Train Epoch: 645 \tLoss: 0.068738\n",
            "Train Epoch: 650 \tLoss: 0.066865\n",
            "\n",
            "Test set: Avg. loss: 0.2550, Accuracy: 458/500 (92%)\n",
            "\n",
            "Train Epoch: 655 \tLoss: 0.066468\n",
            "Train Epoch: 660 \tLoss: 0.065324\n",
            "Train Epoch: 665 \tLoss: 0.065010\n",
            "Train Epoch: 670 \tLoss: 0.063989\n",
            "Train Epoch: 675 \tLoss: 0.063158\n",
            "\n",
            "Test set: Avg. loss: 0.2523, Accuracy: 457/500 (91%)\n",
            "\n",
            "Train Epoch: 680 \tLoss: 0.062181\n",
            "Train Epoch: 685 \tLoss: 0.061535\n",
            "Train Epoch: 690 \tLoss: 0.060873\n",
            "Train Epoch: 695 \tLoss: 0.059977\n",
            "Train Epoch: 700 \tLoss: 0.059360\n",
            "\n",
            "Test set: Avg. loss: 0.2501, Accuracy: 458/500 (92%)\n",
            "\n",
            "Train Epoch: 705 \tLoss: 0.059647\n",
            "Train Epoch: 710 \tLoss: 0.058173\n",
            "Train Epoch: 715 \tLoss: 0.057403\n",
            "Train Epoch: 720 \tLoss: 0.057081\n",
            "Train Epoch: 725 \tLoss: 0.055910\n",
            "\n",
            "Test set: Avg. loss: 0.2473, Accuracy: 458/500 (92%)\n",
            "\n",
            "Train Epoch: 730 \tLoss: 0.055410\n",
            "Train Epoch: 735 \tLoss: 0.054759\n",
            "Train Epoch: 740 \tLoss: 0.054078\n",
            "Train Epoch: 745 \tLoss: 0.053744\n",
            "Train Epoch: 750 \tLoss: 0.052989\n",
            "\n",
            "Test set: Avg. loss: 0.2455, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 755 \tLoss: 0.052333\n",
            "Train Epoch: 760 \tLoss: 0.051766\n",
            "Train Epoch: 765 \tLoss: 0.051115\n",
            "Train Epoch: 770 \tLoss: 0.050560\n",
            "Train Epoch: 775 \tLoss: 0.050021\n",
            "\n",
            "Test set: Avg. loss: 0.2447, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 780 \tLoss: 0.049522\n",
            "Train Epoch: 785 \tLoss: 0.050126\n",
            "Train Epoch: 790 \tLoss: 0.048840\n",
            "Train Epoch: 795 \tLoss: 0.047941\n",
            "Train Epoch: 800 \tLoss: 0.047438\n",
            "\n",
            "Test set: Avg. loss: 0.2412, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 805 \tLoss: 0.049164\n",
            "Train Epoch: 810 \tLoss: 0.047027\n",
            "Train Epoch: 815 \tLoss: 0.045943\n",
            "Train Epoch: 820 \tLoss: 0.045449\n",
            "Train Epoch: 825 \tLoss: 0.048553\n",
            "\n",
            "Test set: Avg. loss: 0.2363, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 830 \tLoss: 0.045900\n",
            "Train Epoch: 835 \tLoss: 0.044710\n",
            "Train Epoch: 840 \tLoss: 0.043765\n",
            "Train Epoch: 845 \tLoss: 0.043189\n",
            "Train Epoch: 850 \tLoss: 0.042707\n",
            "\n",
            "Test set: Avg. loss: 0.2367, Accuracy: 458/500 (92%)\n",
            "\n",
            "Train Epoch: 855 \tLoss: 0.042339\n",
            "Train Epoch: 860 \tLoss: 0.041859\n",
            "Train Epoch: 865 \tLoss: 0.041463\n",
            "Train Epoch: 870 \tLoss: 0.041077\n",
            "Train Epoch: 875 \tLoss: 0.040785\n",
            "\n",
            "Test set: Avg. loss: 0.2367, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 880 \tLoss: 0.040241\n",
            "Train Epoch: 885 \tLoss: 0.039915\n",
            "Train Epoch: 890 \tLoss: 0.039469\n",
            "Train Epoch: 895 \tLoss: 0.039047\n",
            "Train Epoch: 900 \tLoss: 0.038875\n",
            "\n",
            "Test set: Avg. loss: 0.2334, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 905 \tLoss: 0.038369\n",
            "Train Epoch: 910 \tLoss: 0.038080\n",
            "Train Epoch: 915 \tLoss: 0.037752\n",
            "Train Epoch: 920 \tLoss: 0.037390\n",
            "Train Epoch: 925 \tLoss: 0.037026\n",
            "\n",
            "Test set: Avg. loss: 0.2306, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 930 \tLoss: 0.036621\n",
            "Train Epoch: 935 \tLoss: 0.036316\n",
            "Train Epoch: 940 \tLoss: 0.036059\n",
            "Train Epoch: 945 \tLoss: 0.035899\n",
            "Train Epoch: 950 \tLoss: 0.035334\n",
            "\n",
            "Test set: Avg. loss: 0.2278, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 955 \tLoss: 0.035088\n",
            "Train Epoch: 960 \tLoss: 0.034685\n",
            "Train Epoch: 965 \tLoss: 0.034605\n",
            "Train Epoch: 970 \tLoss: 0.034157\n",
            "Train Epoch: 975 \tLoss: 0.033930\n",
            "\n",
            "Test set: Avg. loss: 0.2286, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 980 \tLoss: 0.033611\n",
            "Train Epoch: 985 \tLoss: 0.033450\n",
            "Train Epoch: 990 \tLoss: 0.032979\n",
            "Train Epoch: 995 \tLoss: 0.032710\n",
            "Train Epoch: 1000 \tLoss: 0.032400\n",
            "\n",
            "Test set: Avg. loss: 0.2250, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 1005 \tLoss: 0.032175\n",
            "Train Epoch: 1010 \tLoss: 0.031899\n",
            "Train Epoch: 1015 \tLoss: 0.031684\n",
            "Train Epoch: 1020 \tLoss: 0.031467\n",
            "Train Epoch: 1025 \tLoss: 0.031138\n",
            "\n",
            "Test set: Avg. loss: 0.2239, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 1030 \tLoss: 0.030828\n",
            "Train Epoch: 1035 \tLoss: 0.030640\n",
            "Train Epoch: 1040 \tLoss: 0.030424\n",
            "Train Epoch: 1045 \tLoss: 0.030156\n",
            "Train Epoch: 1050 \tLoss: 0.029808\n",
            "\n",
            "Test set: Avg. loss: 0.2248, Accuracy: 458/500 (92%)\n",
            "\n",
            "Train Epoch: 1055 \tLoss: 0.029721\n",
            "Train Epoch: 1060 \tLoss: 0.029349\n",
            "Train Epoch: 1065 \tLoss: 0.029133\n",
            "Train Epoch: 1070 \tLoss: 0.028898\n",
            "Train Epoch: 1075 \tLoss: 0.029110\n",
            "\n",
            "Test set: Avg. loss: 0.2174, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1080 \tLoss: 0.028547\n",
            "Train Epoch: 1085 \tLoss: 0.028199\n",
            "Train Epoch: 1090 \tLoss: 0.028372\n",
            "Train Epoch: 1095 \tLoss: 0.027907\n",
            "Train Epoch: 1100 \tLoss: 0.027582\n",
            "\n",
            "Test set: Avg. loss: 0.2233, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 1105 \tLoss: 0.027425\n",
            "Train Epoch: 1110 \tLoss: 0.027142\n",
            "Train Epoch: 1115 \tLoss: 0.026934\n",
            "Train Epoch: 1120 \tLoss: 0.026883\n",
            "Train Epoch: 1125 \tLoss: 0.026528\n",
            "\n",
            "Test set: Avg. loss: 0.2217, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 1130 \tLoss: 0.026425\n",
            "Train Epoch: 1135 \tLoss: 0.026191\n",
            "Train Epoch: 1140 \tLoss: 0.026093\n",
            "Train Epoch: 1145 \tLoss: 0.025763\n",
            "Train Epoch: 1150 \tLoss: 0.025654\n",
            "\n",
            "Test set: Avg. loss: 0.2199, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1155 \tLoss: 0.025414\n",
            "Train Epoch: 1160 \tLoss: 0.025200\n",
            "Train Epoch: 1165 \tLoss: 0.025034\n",
            "Train Epoch: 1170 \tLoss: 0.024895\n",
            "Train Epoch: 1175 \tLoss: 0.024717\n",
            "\n",
            "Test set: Avg. loss: 0.2181, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1180 \tLoss: 0.024528\n",
            "Train Epoch: 1185 \tLoss: 0.024360\n",
            "Train Epoch: 1190 \tLoss: 0.024193\n",
            "Train Epoch: 1195 \tLoss: 0.023998\n",
            "Train Epoch: 1200 \tLoss: 0.023864\n",
            "\n",
            "Test set: Avg. loss: 0.2152, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1205 \tLoss: 0.023731\n",
            "Train Epoch: 1210 \tLoss: 0.023507\n",
            "Train Epoch: 1215 \tLoss: 0.023393\n",
            "Train Epoch: 1220 \tLoss: 0.023205\n",
            "Train Epoch: 1225 \tLoss: 0.023041\n",
            "\n",
            "Test set: Avg. loss: 0.2151, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1230 \tLoss: 0.022928\n",
            "Train Epoch: 1235 \tLoss: 0.022749\n",
            "Train Epoch: 1240 \tLoss: 0.022606\n",
            "Train Epoch: 1245 \tLoss: 0.022431\n",
            "Train Epoch: 1250 \tLoss: 0.022325\n",
            "\n",
            "Test set: Avg. loss: 0.2172, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1255 \tLoss: 0.022393\n",
            "Train Epoch: 1260 \tLoss: 0.022014\n",
            "Train Epoch: 1265 \tLoss: 0.021865\n",
            "Train Epoch: 1270 \tLoss: 0.021794\n",
            "Train Epoch: 1275 \tLoss: 0.021632\n",
            "\n",
            "Test set: Avg. loss: 0.2147, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1280 \tLoss: 0.021443\n",
            "Train Epoch: 1285 \tLoss: 0.021327\n",
            "Train Epoch: 1290 \tLoss: 0.021182\n",
            "Train Epoch: 1295 \tLoss: 0.021095\n",
            "Train Epoch: 1300 \tLoss: 0.020918\n",
            "\n",
            "Test set: Avg. loss: 0.2152, Accuracy: 459/500 (92%)\n",
            "\n",
            "Train Epoch: 1305 \tLoss: 0.020826\n",
            "Train Epoch: 1310 \tLoss: 0.020681\n",
            "Train Epoch: 1315 \tLoss: 0.020598\n",
            "Train Epoch: 1320 \tLoss: 0.020544\n",
            "Train Epoch: 1325 \tLoss: 0.020336\n",
            "\n",
            "Test set: Avg. loss: 0.2143, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1330 \tLoss: 0.020358\n",
            "Train Epoch: 1335 \tLoss: 0.020067\n",
            "Train Epoch: 1340 \tLoss: 0.019990\n",
            "Train Epoch: 1345 \tLoss: 0.019876\n",
            "Train Epoch: 1350 \tLoss: 0.019701\n",
            "\n",
            "Test set: Avg. loss: 0.2123, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1355 \tLoss: 0.019586\n",
            "Train Epoch: 1360 \tLoss: 0.019475\n",
            "Train Epoch: 1365 \tLoss: 0.019356\n",
            "Train Epoch: 1370 \tLoss: 0.019248\n",
            "Train Epoch: 1375 \tLoss: 0.019142\n",
            "\n",
            "Test set: Avg. loss: 0.2111, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1380 \tLoss: 0.019060\n",
            "Train Epoch: 1385 \tLoss: 0.018962\n",
            "Train Epoch: 1390 \tLoss: 0.018807\n",
            "Train Epoch: 1395 \tLoss: 0.018750\n",
            "Train Epoch: 1400 \tLoss: 0.018621\n",
            "\n",
            "Test set: Avg. loss: 0.2111, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1405 \tLoss: 0.018478\n",
            "Train Epoch: 1410 \tLoss: 0.018408\n",
            "Train Epoch: 1415 \tLoss: 0.018262\n",
            "Train Epoch: 1420 \tLoss: 0.018234\n",
            "Train Epoch: 1425 \tLoss: 0.018061\n",
            "\n",
            "Test set: Avg. loss: 0.2097, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1430 \tLoss: 0.017992\n",
            "Train Epoch: 1435 \tLoss: 0.017860\n",
            "Train Epoch: 1440 \tLoss: 0.017819\n",
            "Train Epoch: 1445 \tLoss: 0.017669\n",
            "Train Epoch: 1450 \tLoss: 0.017567\n",
            "\n",
            "Test set: Avg. loss: 0.2101, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 1455 \tLoss: 0.017474\n",
            "Train Epoch: 1460 \tLoss: 0.017416\n",
            "Train Epoch: 1465 \tLoss: 0.017297\n",
            "Train Epoch: 1470 \tLoss: 0.017192\n",
            "Train Epoch: 1475 \tLoss: 0.017108\n",
            "\n",
            "Test set: Avg. loss: 0.2085, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1480 \tLoss: 0.017102\n",
            "Train Epoch: 1485 \tLoss: 0.016935\n",
            "Train Epoch: 1490 \tLoss: 0.016829\n",
            "Train Epoch: 1495 \tLoss: 0.016795\n",
            "Train Epoch: 1500 \tLoss: 0.016667\n",
            "\n",
            "Test set: Avg. loss: 0.2087, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1505 \tLoss: 0.016571\n",
            "Train Epoch: 1510 \tLoss: 0.016488\n",
            "Train Epoch: 1515 \tLoss: 0.016399\n",
            "Train Epoch: 1520 \tLoss: 0.016337\n",
            "Train Epoch: 1525 \tLoss: 0.016269\n",
            "\n",
            "Test set: Avg. loss: 0.2090, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1530 \tLoss: 0.016156\n",
            "Train Epoch: 1535 \tLoss: 0.016093\n",
            "Train Epoch: 1540 \tLoss: 0.016027\n",
            "Train Epoch: 1545 \tLoss: 0.015909\n",
            "Train Epoch: 1550 \tLoss: 0.015819\n",
            "\n",
            "Test set: Avg. loss: 0.2088, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 1555 \tLoss: 0.015774\n",
            "Train Epoch: 1560 \tLoss: 0.015674\n",
            "Train Epoch: 1565 \tLoss: 0.015639\n",
            "Train Epoch: 1570 \tLoss: 0.015513\n",
            "Train Epoch: 1575 \tLoss: 0.015449\n",
            "\n",
            "Test set: Avg. loss: 0.2070, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 1580 \tLoss: 0.015386\n",
            "Train Epoch: 1585 \tLoss: 0.015301\n",
            "Train Epoch: 1590 \tLoss: 0.015228\n",
            "Train Epoch: 1595 \tLoss: 0.015374\n",
            "Train Epoch: 1600 \tLoss: 0.015141\n",
            "\n",
            "Test set: Avg. loss: 0.2047, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1605 \tLoss: 0.015018\n",
            "Train Epoch: 1610 \tLoss: 0.014954\n",
            "Train Epoch: 1615 \tLoss: 0.014865\n",
            "Train Epoch: 1620 \tLoss: 0.014801\n",
            "Train Epoch: 1625 \tLoss: 0.014715\n",
            "\n",
            "Test set: Avg. loss: 0.2055, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1630 \tLoss: 0.014653\n",
            "Train Epoch: 1635 \tLoss: 0.014591\n",
            "Train Epoch: 1640 \tLoss: 0.014554\n",
            "Train Epoch: 1645 \tLoss: 0.014474\n",
            "Train Epoch: 1650 \tLoss: 0.014371\n",
            "\n",
            "Test set: Avg. loss: 0.2052, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1655 \tLoss: 0.014329\n",
            "Train Epoch: 1660 \tLoss: 0.014248\n",
            "Train Epoch: 1665 \tLoss: 0.014193\n",
            "Train Epoch: 1670 \tLoss: 0.014156\n",
            "Train Epoch: 1675 \tLoss: 0.014073\n",
            "\n",
            "Test set: Avg. loss: 0.2056, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1680 \tLoss: 0.014020\n",
            "Train Epoch: 1685 \tLoss: 0.013934\n",
            "Train Epoch: 1690 \tLoss: 0.013869\n",
            "Train Epoch: 1695 \tLoss: 0.013817\n",
            "Train Epoch: 1700 \tLoss: 0.013749\n",
            "\n",
            "Test set: Avg. loss: 0.2051, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1705 \tLoss: 0.013674\n",
            "Train Epoch: 1710 \tLoss: 0.013660\n",
            "Train Epoch: 1715 \tLoss: 0.013552\n",
            "Train Epoch: 1720 \tLoss: 0.013502\n",
            "Train Epoch: 1725 \tLoss: 0.013475\n",
            "\n",
            "Test set: Avg. loss: 0.2048, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1730 \tLoss: 0.013383\n",
            "Train Epoch: 1735 \tLoss: 0.013353\n",
            "Train Epoch: 1740 \tLoss: 0.013258\n",
            "Train Epoch: 1745 \tLoss: 0.013205\n",
            "Train Epoch: 1750 \tLoss: 0.013172\n",
            "\n",
            "Test set: Avg. loss: 0.2046, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1755 \tLoss: 0.013090\n",
            "Train Epoch: 1760 \tLoss: 0.013054\n",
            "Train Epoch: 1765 \tLoss: 0.012998\n",
            "Train Epoch: 1770 \tLoss: 0.012914\n",
            "Train Epoch: 1775 \tLoss: 0.012930\n",
            "\n",
            "Test set: Avg. loss: 0.2016, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 1780 \tLoss: 0.012812\n",
            "Train Epoch: 1785 \tLoss: 0.012762\n",
            "Train Epoch: 1790 \tLoss: 0.012701\n",
            "Train Epoch: 1795 \tLoss: 0.012655\n",
            "Train Epoch: 1800 \tLoss: 0.012620\n",
            "\n",
            "Test set: Avg. loss: 0.2036, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1805 \tLoss: 0.012554\n",
            "Train Epoch: 1810 \tLoss: 0.012540\n",
            "Train Epoch: 1815 \tLoss: 0.012469\n",
            "Train Epoch: 1820 \tLoss: 0.012388\n",
            "Train Epoch: 1825 \tLoss: 0.012376\n",
            "\n",
            "Test set: Avg. loss: 0.2039, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 1830 \tLoss: 0.012332\n",
            "Train Epoch: 1835 \tLoss: 0.012255\n",
            "Train Epoch: 1840 \tLoss: 0.012189\n",
            "Train Epoch: 1845 \tLoss: 0.012145\n",
            "Train Epoch: 1850 \tLoss: 0.012098\n",
            "\n",
            "Test set: Avg. loss: 0.2029, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1855 \tLoss: 0.012042\n",
            "Train Epoch: 1860 \tLoss: 0.011990\n",
            "Train Epoch: 1865 \tLoss: 0.011939\n",
            "Train Epoch: 1870 \tLoss: 0.011910\n",
            "Train Epoch: 1875 \tLoss: 0.011878\n",
            "\n",
            "Test set: Avg. loss: 0.2023, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1880 \tLoss: 0.011804\n",
            "Train Epoch: 1885 \tLoss: 0.011845\n",
            "Train Epoch: 1890 \tLoss: 0.011725\n",
            "Train Epoch: 1895 \tLoss: 0.011675\n",
            "Train Epoch: 1900 \tLoss: 0.011618\n",
            "\n",
            "Test set: Avg. loss: 0.2033, Accuracy: 460/500 (92%)\n",
            "\n",
            "Train Epoch: 1905 \tLoss: 0.011573\n",
            "Train Epoch: 1910 \tLoss: 0.011576\n",
            "Train Epoch: 1915 \tLoss: 0.011492\n",
            "Train Epoch: 1920 \tLoss: 0.011445\n",
            "Train Epoch: 1925 \tLoss: 0.011393\n",
            "\n",
            "Test set: Avg. loss: 0.2015, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 1930 \tLoss: 0.011358\n",
            "Train Epoch: 1935 \tLoss: 0.011321\n",
            "Train Epoch: 1940 \tLoss: 0.011270\n",
            "Train Epoch: 1945 \tLoss: 0.011236\n",
            "Train Epoch: 1950 \tLoss: 0.011176\n",
            "\n",
            "Test set: Avg. loss: 0.2025, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 1955 \tLoss: 0.011155\n",
            "Train Epoch: 1960 \tLoss: 0.011099\n",
            "Train Epoch: 1965 \tLoss: 0.011063\n",
            "Train Epoch: 1970 \tLoss: 0.011026\n",
            "Train Epoch: 1975 \tLoss: 0.010969\n",
            "\n",
            "Test set: Avg. loss: 0.2021, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 1980 \tLoss: 0.010929\n",
            "Train Epoch: 1985 \tLoss: 0.010898\n",
            "Train Epoch: 1990 \tLoss: 0.010914\n",
            "Train Epoch: 1995 \tLoss: 0.010816\n",
            "Train Epoch: 2000 \tLoss: 0.010830\n",
            "\n",
            "Test set: Avg. loss: 0.2028, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2005 \tLoss: 0.010738\n",
            "Train Epoch: 2010 \tLoss: 0.010709\n",
            "Train Epoch: 2015 \tLoss: 0.010669\n",
            "Train Epoch: 2020 \tLoss: 0.010644\n",
            "Train Epoch: 2025 \tLoss: 0.010585\n",
            "\n",
            "Test set: Avg. loss: 0.2010, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2030 \tLoss: 0.010567\n",
            "Train Epoch: 2035 \tLoss: 0.010516\n",
            "Train Epoch: 2040 \tLoss: 0.010492\n",
            "Train Epoch: 2045 \tLoss: 0.010434\n",
            "Train Epoch: 2050 \tLoss: 0.010395\n",
            "\n",
            "Test set: Avg. loss: 0.2001, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2055 \tLoss: 0.010363\n",
            "Train Epoch: 2060 \tLoss: 0.010335\n",
            "Train Epoch: 2065 \tLoss: 0.010482\n",
            "Train Epoch: 2070 \tLoss: 0.010337\n",
            "Train Epoch: 2075 \tLoss: 0.010241\n",
            "\n",
            "Test set: Avg. loss: 0.1979, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 2080 \tLoss: 0.010181\n",
            "Train Epoch: 2085 \tLoss: 0.010156\n",
            "Train Epoch: 2090 \tLoss: 0.010108\n",
            "Train Epoch: 2095 \tLoss: 0.010069\n",
            "Train Epoch: 2100 \tLoss: 0.010140\n",
            "\n",
            "Test set: Avg. loss: 0.2008, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2105 \tLoss: 0.010036\n",
            "Train Epoch: 2110 \tLoss: 0.009972\n",
            "Train Epoch: 2115 \tLoss: 0.009939\n",
            "Train Epoch: 2120 \tLoss: 0.009898\n",
            "Train Epoch: 2125 \tLoss: 0.009873\n",
            "\n",
            "Test set: Avg. loss: 0.2006, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2130 \tLoss: 0.009833\n",
            "Train Epoch: 2135 \tLoss: 0.009805\n",
            "Train Epoch: 2140 \tLoss: 0.009783\n",
            "Train Epoch: 2145 \tLoss: 0.009735\n",
            "Train Epoch: 2150 \tLoss: 0.009703\n",
            "\n",
            "Test set: Avg. loss: 0.2013, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2155 \tLoss: 0.009669\n",
            "Train Epoch: 2160 \tLoss: 0.009635\n",
            "Train Epoch: 2165 \tLoss: 0.009610\n",
            "Train Epoch: 2170 \tLoss: 0.009602\n",
            "Train Epoch: 2175 \tLoss: 0.009548\n",
            "\n",
            "Test set: Avg. loss: 0.1998, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 2180 \tLoss: 0.009539\n",
            "Train Epoch: 2185 \tLoss: 0.009481\n",
            "Train Epoch: 2190 \tLoss: 0.009490\n",
            "Train Epoch: 2195 \tLoss: 0.009423\n",
            "Train Epoch: 2200 \tLoss: 0.009386\n",
            "\n",
            "Test set: Avg. loss: 0.1998, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2205 \tLoss: 0.009363\n",
            "Train Epoch: 2210 \tLoss: 0.009326\n",
            "Train Epoch: 2215 \tLoss: 0.009312\n",
            "Train Epoch: 2220 \tLoss: 0.009268\n",
            "Train Epoch: 2225 \tLoss: 0.009237\n",
            "\n",
            "Test set: Avg. loss: 0.2016, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2230 \tLoss: 0.009209\n",
            "Train Epoch: 2235 \tLoss: 0.009190\n",
            "Train Epoch: 2240 \tLoss: 0.009155\n",
            "Train Epoch: 2245 \tLoss: 0.009126\n",
            "Train Epoch: 2250 \tLoss: 0.009094\n",
            "\n",
            "Test set: Avg. loss: 0.1997, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 2255 \tLoss: 0.009064\n",
            "Train Epoch: 2260 \tLoss: 0.009067\n",
            "Train Epoch: 2265 \tLoss: 0.009018\n",
            "Train Epoch: 2270 \tLoss: 0.008995\n",
            "Train Epoch: 2275 \tLoss: 0.008955\n",
            "\n",
            "Test set: Avg. loss: 0.1996, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 2280 \tLoss: 0.008942\n",
            "Train Epoch: 2285 \tLoss: 0.008903\n",
            "Train Epoch: 2290 \tLoss: 0.008865\n",
            "Train Epoch: 2295 \tLoss: 0.008857\n",
            "Train Epoch: 2300 \tLoss: 0.008817\n",
            "\n",
            "Test set: Avg. loss: 0.1992, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2305 \tLoss: 0.008789\n",
            "Train Epoch: 2310 \tLoss: 0.008762\n",
            "Train Epoch: 2315 \tLoss: 0.008747\n",
            "Train Epoch: 2320 \tLoss: 0.008707\n",
            "Train Epoch: 2325 \tLoss: 0.008705\n",
            "\n",
            "Test set: Avg. loss: 0.1972, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2330 \tLoss: 0.008656\n",
            "Train Epoch: 2335 \tLoss: 0.008630\n",
            "Train Epoch: 2340 \tLoss: 0.008611\n",
            "Train Epoch: 2345 \tLoss: 0.008603\n",
            "Train Epoch: 2350 \tLoss: 0.008550\n",
            "\n",
            "Test set: Avg. loss: 0.1983, Accuracy: 461/500 (92%)\n",
            "\n",
            "Train Epoch: 2355 \tLoss: 0.008529\n",
            "Train Epoch: 2360 \tLoss: 0.008502\n",
            "Train Epoch: 2365 \tLoss: 0.008474\n",
            "Train Epoch: 2370 \tLoss: 0.008450\n",
            "Train Epoch: 2375 \tLoss: 0.008519\n",
            "\n",
            "Test set: Avg. loss: 0.2030, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2380 \tLoss: 0.008449\n",
            "Train Epoch: 2385 \tLoss: 0.008396\n",
            "Train Epoch: 2390 \tLoss: 0.008370\n",
            "Train Epoch: 2395 \tLoss: 0.008343\n",
            "Train Epoch: 2400 \tLoss: 0.008315\n",
            "\n",
            "Test set: Avg. loss: 0.1990, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 2405 \tLoss: 0.008285\n",
            "Train Epoch: 2410 \tLoss: 0.008274\n",
            "Train Epoch: 2415 \tLoss: 0.008238\n",
            "Train Epoch: 2420 \tLoss: 0.008212\n",
            "Train Epoch: 2425 \tLoss: 0.008190\n",
            "\n",
            "Test set: Avg. loss: 0.1990, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2430 \tLoss: 0.008163\n",
            "Train Epoch: 2435 \tLoss: 0.008173\n",
            "Train Epoch: 2440 \tLoss: 0.008118\n",
            "Train Epoch: 2445 \tLoss: 0.008111\n",
            "Train Epoch: 2450 \tLoss: 0.008077\n",
            "\n",
            "Test set: Avg. loss: 0.1993, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2455 \tLoss: 0.008064\n",
            "Train Epoch: 2460 \tLoss: 0.008029\n",
            "Train Epoch: 2465 \tLoss: 0.008007\n",
            "Train Epoch: 2470 \tLoss: 0.007990\n",
            "Train Epoch: 2475 \tLoss: 0.007956\n",
            "\n",
            "Test set: Avg. loss: 0.1977, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2480 \tLoss: 0.007944\n",
            "Train Epoch: 2485 \tLoss: 0.007914\n",
            "Train Epoch: 2490 \tLoss: 0.007902\n",
            "Train Epoch: 2495 \tLoss: 0.007869\n",
            "Train Epoch: 2500 \tLoss: 0.007848\n",
            "\n",
            "Test set: Avg. loss: 0.1980, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2505 \tLoss: 0.007835\n",
            "Train Epoch: 2510 \tLoss: 0.007811\n",
            "Train Epoch: 2515 \tLoss: 0.007781\n",
            "Train Epoch: 2520 \tLoss: 0.007760\n",
            "Train Epoch: 2525 \tLoss: 0.007762\n",
            "\n",
            "Test set: Avg. loss: 0.1961, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2530 \tLoss: 0.007718\n",
            "Train Epoch: 2535 \tLoss: 0.007706\n",
            "Train Epoch: 2540 \tLoss: 0.007677\n",
            "Train Epoch: 2545 \tLoss: 0.007650\n",
            "Train Epoch: 2550 \tLoss: 0.007650\n",
            "\n",
            "Test set: Avg. loss: 0.1979, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2555 \tLoss: 0.007612\n",
            "Train Epoch: 2560 \tLoss: 0.007592\n",
            "Train Epoch: 2565 \tLoss: 0.007615\n",
            "Train Epoch: 2570 \tLoss: 0.007564\n",
            "Train Epoch: 2575 \tLoss: 0.007532\n",
            "\n",
            "Test set: Avg. loss: 0.1976, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2580 \tLoss: 0.007516\n",
            "Train Epoch: 2585 \tLoss: 0.007496\n",
            "Train Epoch: 2590 \tLoss: 0.007473\n",
            "Train Epoch: 2595 \tLoss: 0.007459\n",
            "Train Epoch: 2600 \tLoss: 0.007435\n",
            "\n",
            "Test set: Avg. loss: 0.1974, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2605 \tLoss: 0.007411\n",
            "Train Epoch: 2610 \tLoss: 0.007392\n",
            "Train Epoch: 2615 \tLoss: 0.007377\n",
            "Train Epoch: 2620 \tLoss: 0.007380\n",
            "Train Epoch: 2625 \tLoss: 0.007338\n",
            "\n",
            "Test set: Avg. loss: 0.1969, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2630 \tLoss: 0.007317\n",
            "Train Epoch: 2635 \tLoss: 0.007300\n",
            "Train Epoch: 2640 \tLoss: 0.007280\n",
            "Train Epoch: 2645 \tLoss: 0.007257\n",
            "Train Epoch: 2650 \tLoss: 0.007244\n",
            "\n",
            "Test set: Avg. loss: 0.1970, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2655 \tLoss: 0.007226\n",
            "Train Epoch: 2660 \tLoss: 0.007205\n",
            "Train Epoch: 2665 \tLoss: 0.007184\n",
            "Train Epoch: 2670 \tLoss: 0.007164\n",
            "Train Epoch: 2675 \tLoss: 0.007156\n",
            "\n",
            "Test set: Avg. loss: 0.1971, Accuracy: 462/500 (92%)\n",
            "\n",
            "Train Epoch: 2680 \tLoss: 0.007126\n",
            "Train Epoch: 2685 \tLoss: 0.007108\n",
            "Train Epoch: 2690 \tLoss: 0.007097\n",
            "Train Epoch: 2695 \tLoss: 0.007075\n",
            "Train Epoch: 2700 \tLoss: 0.007055\n",
            "\n",
            "Test set: Avg. loss: 0.1969, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2705 \tLoss: 0.007050\n",
            "Train Epoch: 2710 \tLoss: 0.007019\n",
            "Train Epoch: 2715 \tLoss: 0.007012\n",
            "Train Epoch: 2720 \tLoss: 0.006985\n",
            "Train Epoch: 2725 \tLoss: 0.006974\n",
            "\n",
            "Test set: Avg. loss: 0.1970, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2730 \tLoss: 0.006960\n",
            "Train Epoch: 2735 \tLoss: 0.006932\n",
            "Train Epoch: 2740 \tLoss: 0.006915\n",
            "Train Epoch: 2745 \tLoss: 0.006915\n",
            "Train Epoch: 2750 \tLoss: 0.006879\n",
            "\n",
            "Test set: Avg. loss: 0.1960, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2755 \tLoss: 0.006863\n",
            "Train Epoch: 2760 \tLoss: 0.006847\n",
            "Train Epoch: 2765 \tLoss: 0.006827\n",
            "Train Epoch: 2770 \tLoss: 0.006810\n",
            "Train Epoch: 2775 \tLoss: 0.006795\n",
            "\n",
            "Test set: Avg. loss: 0.1961, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2780 \tLoss: 0.006779\n",
            "Train Epoch: 2785 \tLoss: 0.006761\n",
            "Train Epoch: 2790 \tLoss: 0.006746\n",
            "Train Epoch: 2795 \tLoss: 0.006740\n",
            "Train Epoch: 2800 \tLoss: 0.006713\n",
            "\n",
            "Test set: Avg. loss: 0.1965, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2805 \tLoss: 0.006703\n",
            "Train Epoch: 2810 \tLoss: 0.006692\n",
            "Train Epoch: 2815 \tLoss: 0.006662\n",
            "Train Epoch: 2820 \tLoss: 0.006663\n",
            "Train Epoch: 2825 \tLoss: 0.006636\n",
            "\n",
            "Test set: Avg. loss: 0.1965, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2830 \tLoss: 0.006618\n",
            "Train Epoch: 2835 \tLoss: 0.006600\n",
            "Train Epoch: 2840 \tLoss: 0.006594\n",
            "Train Epoch: 2845 \tLoss: 0.006567\n",
            "Train Epoch: 2850 \tLoss: 0.006581\n",
            "\n",
            "Test set: Avg. loss: 0.1982, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2855 \tLoss: 0.006543\n",
            "Train Epoch: 2860 \tLoss: 0.006543\n",
            "Train Epoch: 2865 \tLoss: 0.006509\n",
            "Train Epoch: 2870 \tLoss: 0.006490\n",
            "Train Epoch: 2875 \tLoss: 0.006498\n",
            "\n",
            "Test set: Avg. loss: 0.1961, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 2880 \tLoss: 0.006461\n",
            "Train Epoch: 2885 \tLoss: 0.006445\n",
            "Train Epoch: 2890 \tLoss: 0.006448\n",
            "Train Epoch: 2895 \tLoss: 0.006423\n",
            "Train Epoch: 2900 \tLoss: 0.006403\n",
            "\n",
            "Test set: Avg. loss: 0.1965, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2905 \tLoss: 0.006384\n",
            "Train Epoch: 2910 \tLoss: 0.006370\n",
            "Train Epoch: 2915 \tLoss: 0.006354\n",
            "Train Epoch: 2920 \tLoss: 0.006349\n",
            "Train Epoch: 2925 \tLoss: 0.006324\n",
            "\n",
            "Test set: Avg. loss: 0.1959, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 2930 \tLoss: 0.006321\n",
            "Train Epoch: 2935 \tLoss: 0.006299\n",
            "Train Epoch: 2940 \tLoss: 0.006280\n",
            "Train Epoch: 2945 \tLoss: 0.006270\n",
            "Train Epoch: 2950 \tLoss: 0.006252\n",
            "\n",
            "Test set: Avg. loss: 0.1962, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 2955 \tLoss: 0.006239\n",
            "Train Epoch: 2960 \tLoss: 0.006223\n",
            "Train Epoch: 2965 \tLoss: 0.006211\n",
            "Train Epoch: 2970 \tLoss: 0.006208\n",
            "Train Epoch: 2975 \tLoss: 0.006184\n",
            "\n",
            "Test set: Avg. loss: 0.1959, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 2980 \tLoss: 0.006169\n",
            "Train Epoch: 2985 \tLoss: 0.006156\n",
            "Train Epoch: 2990 \tLoss: 0.006145\n",
            "Train Epoch: 2995 \tLoss: 0.006127\n",
            "Train Epoch: 3000 \tLoss: 0.006122\n",
            "\n",
            "Test set: Avg. loss: 0.1955, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3005 \tLoss: 0.006101\n",
            "Train Epoch: 3010 \tLoss: 0.006085\n",
            "Train Epoch: 3015 \tLoss: 0.006071\n",
            "Train Epoch: 3020 \tLoss: 0.006060\n",
            "Train Epoch: 3025 \tLoss: 0.006053\n",
            "\n",
            "Test set: Avg. loss: 0.1945, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 3030 \tLoss: 0.006031\n",
            "Train Epoch: 3035 \tLoss: 0.006023\n",
            "Train Epoch: 3040 \tLoss: 0.006004\n",
            "Train Epoch: 3045 \tLoss: 0.006036\n",
            "Train Epoch: 3050 \tLoss: 0.005996\n",
            "\n",
            "Test set: Avg. loss: 0.1958, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 3055 \tLoss: 0.005971\n",
            "Train Epoch: 3060 \tLoss: 0.005958\n",
            "Train Epoch: 3065 \tLoss: 0.005943\n",
            "Train Epoch: 3070 \tLoss: 0.005932\n",
            "Train Epoch: 3075 \tLoss: 0.005915\n",
            "\n",
            "Test set: Avg. loss: 0.1958, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3080 \tLoss: 0.005903\n",
            "Train Epoch: 3085 \tLoss: 0.005887\n",
            "Train Epoch: 3090 \tLoss: 0.005875\n",
            "Train Epoch: 3095 \tLoss: 0.005864\n",
            "Train Epoch: 3100 \tLoss: 0.005855\n",
            "\n",
            "Test set: Avg. loss: 0.1960, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3105 \tLoss: 0.005872\n",
            "Train Epoch: 3110 \tLoss: 0.005836\n",
            "Train Epoch: 3115 \tLoss: 0.005814\n",
            "Train Epoch: 3120 \tLoss: 0.005800\n",
            "Train Epoch: 3125 \tLoss: 0.005803\n",
            "\n",
            "Test set: Avg. loss: 0.1944, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3130 \tLoss: 0.005779\n",
            "Train Epoch: 3135 \tLoss: 0.005770\n",
            "Train Epoch: 3140 \tLoss: 0.005755\n",
            "Train Epoch: 3145 \tLoss: 0.005741\n",
            "Train Epoch: 3150 \tLoss: 0.005728\n",
            "\n",
            "Test set: Avg. loss: 0.1951, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3155 \tLoss: 0.005717\n",
            "Train Epoch: 3160 \tLoss: 0.005704\n",
            "Train Epoch: 3165 \tLoss: 0.005697\n",
            "Train Epoch: 3170 \tLoss: 0.005679\n",
            "Train Epoch: 3175 \tLoss: 0.005667\n",
            "\n",
            "Test set: Avg. loss: 0.1951, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3180 \tLoss: 0.005656\n",
            "Train Epoch: 3185 \tLoss: 0.005643\n",
            "Train Epoch: 3190 \tLoss: 0.005631\n",
            "Train Epoch: 3195 \tLoss: 0.005619\n",
            "Train Epoch: 3200 \tLoss: 0.005622\n",
            "\n",
            "Test set: Avg. loss: 0.1948, Accuracy: 463/500 (93%)\n",
            "\n",
            "Train Epoch: 3205 \tLoss: 0.005599\n",
            "Train Epoch: 3210 \tLoss: 0.005586\n",
            "Train Epoch: 3215 \tLoss: 0.005576\n",
            "Train Epoch: 3220 \tLoss: 0.005560\n",
            "Train Epoch: 3225 \tLoss: 0.005552\n",
            "\n",
            "Test set: Avg. loss: 0.1952, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3230 \tLoss: 0.005540\n",
            "Train Epoch: 3235 \tLoss: 0.005528\n",
            "Train Epoch: 3240 \tLoss: 0.005520\n",
            "Train Epoch: 3245 \tLoss: 0.005504\n",
            "Train Epoch: 3250 \tLoss: 0.005491\n",
            "\n",
            "Test set: Avg. loss: 0.1948, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3255 \tLoss: 0.005480\n",
            "Train Epoch: 3260 \tLoss: 0.005471\n",
            "Train Epoch: 3265 \tLoss: 0.005459\n",
            "Train Epoch: 3270 \tLoss: 0.005446\n",
            "Train Epoch: 3275 \tLoss: 0.005436\n",
            "\n",
            "Test set: Avg. loss: 0.1947, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3280 \tLoss: 0.005425\n",
            "Train Epoch: 3285 \tLoss: 0.005414\n",
            "Train Epoch: 3290 \tLoss: 0.005402\n",
            "Train Epoch: 3295 \tLoss: 0.005392\n",
            "Train Epoch: 3300 \tLoss: 0.005382\n",
            "\n",
            "Test set: Avg. loss: 0.1947, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3305 \tLoss: 0.005371\n",
            "Train Epoch: 3310 \tLoss: 0.005366\n",
            "Train Epoch: 3315 \tLoss: 0.005350\n",
            "Train Epoch: 3320 \tLoss: 0.005337\n",
            "Train Epoch: 3325 \tLoss: 0.005327\n",
            "\n",
            "Test set: Avg. loss: 0.1943, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3330 \tLoss: 0.005317\n",
            "Train Epoch: 3335 \tLoss: 0.005312\n",
            "Train Epoch: 3340 \tLoss: 0.005295\n",
            "Train Epoch: 3345 \tLoss: 0.005288\n",
            "Train Epoch: 3350 \tLoss: 0.005274\n",
            "\n",
            "Test set: Avg. loss: 0.1942, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3355 \tLoss: 0.005264\n",
            "Train Epoch: 3360 \tLoss: 0.005254\n",
            "Train Epoch: 3365 \tLoss: 0.005243\n",
            "Train Epoch: 3370 \tLoss: 0.005232\n",
            "Train Epoch: 3375 \tLoss: 0.005228\n",
            "\n",
            "Test set: Avg. loss: 0.1950, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3380 \tLoss: 0.005211\n",
            "Train Epoch: 3385 \tLoss: 0.005200\n",
            "Train Epoch: 3390 \tLoss: 0.005195\n",
            "Train Epoch: 3395 \tLoss: 0.005181\n",
            "Train Epoch: 3400 \tLoss: 0.005170\n",
            "\n",
            "Test set: Avg. loss: 0.1939, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3405 \tLoss: 0.005163\n",
            "Train Epoch: 3410 \tLoss: 0.005152\n",
            "Train Epoch: 3415 \tLoss: 0.005148\n",
            "Train Epoch: 3420 \tLoss: 0.005132\n",
            "Train Epoch: 3425 \tLoss: 0.005121\n",
            "\n",
            "Test set: Avg. loss: 0.1949, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3430 \tLoss: 0.005113\n",
            "Train Epoch: 3435 \tLoss: 0.005100\n",
            "Train Epoch: 3440 \tLoss: 0.005098\n",
            "Train Epoch: 3445 \tLoss: 0.005082\n",
            "Train Epoch: 3450 \tLoss: 0.005071\n",
            "\n",
            "Test set: Avg. loss: 0.1947, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3455 \tLoss: 0.005061\n",
            "Train Epoch: 3460 \tLoss: 0.005052\n",
            "Train Epoch: 3465 \tLoss: 0.005042\n",
            "Train Epoch: 3470 \tLoss: 0.005033\n",
            "Train Epoch: 3475 \tLoss: 0.005026\n",
            "\n",
            "Test set: Avg. loss: 0.1938, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3480 \tLoss: 0.005013\n",
            "Train Epoch: 3485 \tLoss: 0.005006\n",
            "Train Epoch: 3490 \tLoss: 0.004995\n",
            "Train Epoch: 3495 \tLoss: 0.004985\n",
            "Train Epoch: 3500 \tLoss: 0.004975\n",
            "\n",
            "Test set: Avg. loss: 0.1941, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3505 \tLoss: 0.004970\n",
            "Train Epoch: 3510 \tLoss: 0.004956\n",
            "Train Epoch: 3515 \tLoss: 0.004955\n",
            "Train Epoch: 3520 \tLoss: 0.004939\n",
            "Train Epoch: 3525 \tLoss: 0.004926\n",
            "\n",
            "Test set: Avg. loss: 0.1931, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3530 \tLoss: 0.004922\n",
            "Train Epoch: 3535 \tLoss: 0.004909\n",
            "Train Epoch: 3540 \tLoss: 0.004899\n",
            "Train Epoch: 3545 \tLoss: 0.004890\n",
            "Train Epoch: 3550 \tLoss: 0.004880\n",
            "\n",
            "Test set: Avg. loss: 0.1916, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3555 \tLoss: 0.004875\n",
            "Train Epoch: 3560 \tLoss: 0.004863\n",
            "Train Epoch: 3565 \tLoss: 0.004856\n",
            "Train Epoch: 3570 \tLoss: 0.004844\n",
            "Train Epoch: 3575 \tLoss: 0.004835\n",
            "\n",
            "Test set: Avg. loss: 0.1943, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3580 \tLoss: 0.004826\n",
            "Train Epoch: 3585 \tLoss: 0.004817\n",
            "Train Epoch: 3590 \tLoss: 0.004810\n",
            "Train Epoch: 3595 \tLoss: 0.004803\n",
            "Train Epoch: 3600 \tLoss: 0.004797\n",
            "\n",
            "Test set: Avg. loss: 0.1943, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 3605 \tLoss: 0.004782\n",
            "Train Epoch: 3610 \tLoss: 0.004773\n",
            "Train Epoch: 3615 \tLoss: 0.004766\n",
            "Train Epoch: 3620 \tLoss: 0.004774\n",
            "Train Epoch: 3625 \tLoss: 0.004754\n",
            "\n",
            "Test set: Avg. loss: 0.1936, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3630 \tLoss: 0.004739\n",
            "Train Epoch: 3635 \tLoss: 0.004735\n",
            "Train Epoch: 3640 \tLoss: 0.004725\n",
            "Train Epoch: 3645 \tLoss: 0.004713\n",
            "Train Epoch: 3650 \tLoss: 0.004706\n",
            "\n",
            "Test set: Avg. loss: 0.1940, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3655 \tLoss: 0.004696\n",
            "Train Epoch: 3660 \tLoss: 0.004688\n",
            "Train Epoch: 3665 \tLoss: 0.004680\n",
            "Train Epoch: 3670 \tLoss: 0.004672\n",
            "Train Epoch: 3675 \tLoss: 0.004666\n",
            "\n",
            "Test set: Avg. loss: 0.1945, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3680 \tLoss: 0.004654\n",
            "Train Epoch: 3685 \tLoss: 0.004646\n",
            "Train Epoch: 3690 \tLoss: 0.004638\n",
            "Train Epoch: 3695 \tLoss: 0.004631\n",
            "Train Epoch: 3700 \tLoss: 0.004620\n",
            "\n",
            "Test set: Avg. loss: 0.1929, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3705 \tLoss: 0.004614\n",
            "Train Epoch: 3710 \tLoss: 0.004620\n",
            "Train Epoch: 3715 \tLoss: 0.004601\n",
            "Train Epoch: 3720 \tLoss: 0.004591\n",
            "Train Epoch: 3725 \tLoss: 0.004583\n",
            "\n",
            "Test set: Avg. loss: 0.1940, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3730 \tLoss: 0.004577\n",
            "Train Epoch: 3735 \tLoss: 0.004567\n",
            "Train Epoch: 3740 \tLoss: 0.004558\n",
            "Train Epoch: 3745 \tLoss: 0.004550\n",
            "Train Epoch: 3750 \tLoss: 0.004540\n",
            "\n",
            "Test set: Avg. loss: 0.1951, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3755 \tLoss: 0.004536\n",
            "Train Epoch: 3760 \tLoss: 0.004527\n",
            "Train Epoch: 3765 \tLoss: 0.004522\n",
            "Train Epoch: 3770 \tLoss: 0.004511\n",
            "Train Epoch: 3775 \tLoss: 0.004505\n",
            "\n",
            "Test set: Avg. loss: 0.1936, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3780 \tLoss: 0.004495\n",
            "Train Epoch: 3785 \tLoss: 0.004487\n",
            "Train Epoch: 3790 \tLoss: 0.004480\n",
            "Train Epoch: 3795 \tLoss: 0.004473\n",
            "Train Epoch: 3800 \tLoss: 0.004462\n",
            "\n",
            "Test set: Avg. loss: 0.1941, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 3805 \tLoss: 0.004455\n",
            "Train Epoch: 3810 \tLoss: 0.004459\n",
            "Train Epoch: 3815 \tLoss: 0.004442\n",
            "Train Epoch: 3820 \tLoss: 0.004432\n",
            "Train Epoch: 3825 \tLoss: 0.004425\n",
            "\n",
            "Test set: Avg. loss: 0.1933, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3830 \tLoss: 0.004418\n",
            "Train Epoch: 3835 \tLoss: 0.004411\n",
            "Train Epoch: 3840 \tLoss: 0.004413\n",
            "Train Epoch: 3845 \tLoss: 0.004396\n",
            "Train Epoch: 3850 \tLoss: 0.004388\n",
            "\n",
            "Test set: Avg. loss: 0.1932, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3855 \tLoss: 0.004382\n",
            "Train Epoch: 3860 \tLoss: 0.004373\n",
            "Train Epoch: 3865 \tLoss: 0.004370\n",
            "Train Epoch: 3870 \tLoss: 0.004360\n",
            "Train Epoch: 3875 \tLoss: 0.004351\n",
            "\n",
            "Test set: Avg. loss: 0.1947, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3880 \tLoss: 0.004344\n",
            "Train Epoch: 3885 \tLoss: 0.004337\n",
            "Train Epoch: 3890 \tLoss: 0.004329\n",
            "Train Epoch: 3895 \tLoss: 0.004335\n",
            "Train Epoch: 3900 \tLoss: 0.004321\n",
            "\n",
            "Test set: Avg. loss: 0.1933, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 3905 \tLoss: 0.004310\n",
            "Train Epoch: 3910 \tLoss: 0.004302\n",
            "Train Epoch: 3915 \tLoss: 0.004295\n",
            "Train Epoch: 3920 \tLoss: 0.004287\n",
            "Train Epoch: 3925 \tLoss: 0.004280\n",
            "\n",
            "Test set: Avg. loss: 0.1928, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 3930 \tLoss: 0.004273\n",
            "Train Epoch: 3935 \tLoss: 0.004267\n",
            "Train Epoch: 3940 \tLoss: 0.004260\n",
            "Train Epoch: 3945 \tLoss: 0.004252\n",
            "Train Epoch: 3950 \tLoss: 0.004246\n",
            "\n",
            "Test set: Avg. loss: 0.1931, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 3955 \tLoss: 0.004239\n",
            "Train Epoch: 3960 \tLoss: 0.004231\n",
            "Train Epoch: 3965 \tLoss: 0.004228\n",
            "Train Epoch: 3970 \tLoss: 0.004219\n",
            "Train Epoch: 3975 \tLoss: 0.004216\n",
            "\n",
            "Test set: Avg. loss: 0.1921, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 3980 \tLoss: 0.004203\n",
            "Train Epoch: 3985 \tLoss: 0.004196\n",
            "Train Epoch: 3990 \tLoss: 0.004198\n",
            "Train Epoch: 3995 \tLoss: 0.004192\n",
            "Train Epoch: 4000 \tLoss: 0.004179\n",
            "\n",
            "Test set: Avg. loss: 0.1930, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4005 \tLoss: 0.004170\n",
            "Train Epoch: 4010 \tLoss: 0.004164\n",
            "Train Epoch: 4015 \tLoss: 0.004157\n",
            "Train Epoch: 4020 \tLoss: 0.004150\n",
            "Train Epoch: 4025 \tLoss: 0.004144\n",
            "\n",
            "Test set: Avg. loss: 0.1929, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4030 \tLoss: 0.004138\n",
            "Train Epoch: 4035 \tLoss: 0.004130\n",
            "Train Epoch: 4040 \tLoss: 0.004123\n",
            "Train Epoch: 4045 \tLoss: 0.004117\n",
            "Train Epoch: 4050 \tLoss: 0.004110\n",
            "\n",
            "Test set: Avg. loss: 0.1931, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4055 \tLoss: 0.004103\n",
            "Train Epoch: 4060 \tLoss: 0.004100\n",
            "Train Epoch: 4065 \tLoss: 0.004094\n",
            "Train Epoch: 4070 \tLoss: 0.004083\n",
            "Train Epoch: 4075 \tLoss: 0.004092\n",
            "\n",
            "Test set: Avg. loss: 0.1945, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4080 \tLoss: 0.004079\n",
            "Train Epoch: 4085 \tLoss: 0.004066\n",
            "Train Epoch: 4090 \tLoss: 0.004060\n",
            "Train Epoch: 4095 \tLoss: 0.004053\n",
            "Train Epoch: 4100 \tLoss: 0.004048\n",
            "\n",
            "Test set: Avg. loss: 0.1927, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4105 \tLoss: 0.004039\n",
            "Train Epoch: 4110 \tLoss: 0.004035\n",
            "Train Epoch: 4115 \tLoss: 0.004038\n",
            "Train Epoch: 4120 \tLoss: 0.004025\n",
            "Train Epoch: 4125 \tLoss: 0.004018\n",
            "\n",
            "Test set: Avg. loss: 0.1924, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4130 \tLoss: 0.004010\n",
            "Train Epoch: 4135 \tLoss: 0.004003\n",
            "Train Epoch: 4140 \tLoss: 0.003996\n",
            "Train Epoch: 4145 \tLoss: 0.003992\n",
            "Train Epoch: 4150 \tLoss: 0.003984\n",
            "\n",
            "Test set: Avg. loss: 0.1931, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4155 \tLoss: 0.003978\n",
            "Train Epoch: 4160 \tLoss: 0.003973\n",
            "Train Epoch: 4165 \tLoss: 0.003966\n",
            "Train Epoch: 4170 \tLoss: 0.003959\n",
            "Train Epoch: 4175 \tLoss: 0.003955\n",
            "\n",
            "Test set: Avg. loss: 0.1928, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4180 \tLoss: 0.003951\n",
            "Train Epoch: 4185 \tLoss: 0.003944\n",
            "Train Epoch: 4190 \tLoss: 0.003935\n",
            "Train Epoch: 4195 \tLoss: 0.003931\n",
            "Train Epoch: 4200 \tLoss: 0.003926\n",
            "\n",
            "Test set: Avg. loss: 0.1921, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4205 \tLoss: 0.003924\n",
            "Train Epoch: 4210 \tLoss: 0.003911\n",
            "Train Epoch: 4215 \tLoss: 0.003906\n",
            "Train Epoch: 4220 \tLoss: 0.003903\n",
            "Train Epoch: 4225 \tLoss: 0.003894\n",
            "\n",
            "Test set: Avg. loss: 0.1925, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4230 \tLoss: 0.003888\n",
            "Train Epoch: 4235 \tLoss: 0.003882\n",
            "Train Epoch: 4240 \tLoss: 0.003875\n",
            "Train Epoch: 4245 \tLoss: 0.003870\n",
            "Train Epoch: 4250 \tLoss: 0.003863\n",
            "\n",
            "Test set: Avg. loss: 0.1919, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4255 \tLoss: 0.003858\n",
            "Train Epoch: 4260 \tLoss: 0.003853\n",
            "Train Epoch: 4265 \tLoss: 0.003846\n",
            "Train Epoch: 4270 \tLoss: 0.003840\n",
            "Train Epoch: 4275 \tLoss: 0.003834\n",
            "\n",
            "Test set: Avg. loss: 0.1912, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4280 \tLoss: 0.003829\n",
            "Train Epoch: 4285 \tLoss: 0.003823\n",
            "Train Epoch: 4290 \tLoss: 0.003823\n",
            "Train Epoch: 4295 \tLoss: 0.003811\n",
            "Train Epoch: 4300 \tLoss: 0.003805\n",
            "\n",
            "Test set: Avg. loss: 0.1938, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4305 \tLoss: 0.003800\n",
            "Train Epoch: 4310 \tLoss: 0.003795\n",
            "Train Epoch: 4315 \tLoss: 0.003790\n",
            "Train Epoch: 4320 \tLoss: 0.003784\n",
            "Train Epoch: 4325 \tLoss: 0.003778\n",
            "\n",
            "Test set: Avg. loss: 0.1923, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4330 \tLoss: 0.003773\n",
            "Train Epoch: 4335 \tLoss: 0.003767\n",
            "Train Epoch: 4340 \tLoss: 0.003764\n",
            "Train Epoch: 4345 \tLoss: 0.003756\n",
            "Train Epoch: 4350 \tLoss: 0.003750\n",
            "\n",
            "Test set: Avg. loss: 0.1925, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4355 \tLoss: 0.003743\n",
            "Train Epoch: 4360 \tLoss: 0.003739\n",
            "Train Epoch: 4365 \tLoss: 0.003734\n",
            "Train Epoch: 4370 \tLoss: 0.003729\n",
            "Train Epoch: 4375 \tLoss: 0.003724\n",
            "\n",
            "Test set: Avg. loss: 0.1929, Accuracy: 464/500 (93%)\n",
            "\n",
            "Train Epoch: 4380 \tLoss: 0.003717\n",
            "Train Epoch: 4385 \tLoss: 0.003711\n",
            "Train Epoch: 4390 \tLoss: 0.003707\n",
            "Train Epoch: 4395 \tLoss: 0.003703\n",
            "Train Epoch: 4400 \tLoss: 0.003699\n",
            "\n",
            "Test set: Avg. loss: 0.1925, Accuracy: 467/500 (93%)\n",
            "\n",
            "Train Epoch: 4405 \tLoss: 0.003690\n",
            "Train Epoch: 4410 \tLoss: 0.003689\n",
            "Train Epoch: 4415 \tLoss: 0.003680\n",
            "Train Epoch: 4420 \tLoss: 0.003678\n",
            "Train Epoch: 4425 \tLoss: 0.003669\n",
            "\n",
            "Test set: Avg. loss: 0.1922, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4430 \tLoss: 0.003663\n",
            "Train Epoch: 4435 \tLoss: 0.003658\n",
            "Train Epoch: 4440 \tLoss: 0.003654\n",
            "Train Epoch: 4445 \tLoss: 0.003648\n",
            "Train Epoch: 4450 \tLoss: 0.003644\n",
            "\n",
            "Test set: Avg. loss: 0.1922, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4455 \tLoss: 0.003637\n",
            "Train Epoch: 4460 \tLoss: 0.003636\n",
            "Train Epoch: 4465 \tLoss: 0.003628\n",
            "Train Epoch: 4470 \tLoss: 0.003622\n",
            "Train Epoch: 4475 \tLoss: 0.003618\n",
            "\n",
            "Test set: Avg. loss: 0.1933, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4480 \tLoss: 0.003612\n",
            "Train Epoch: 4485 \tLoss: 0.003608\n",
            "Train Epoch: 4490 \tLoss: 0.003601\n",
            "Train Epoch: 4495 \tLoss: 0.003596\n",
            "Train Epoch: 4500 \tLoss: 0.003590\n",
            "\n",
            "Test set: Avg. loss: 0.1927, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4505 \tLoss: 0.003591\n",
            "Train Epoch: 4510 \tLoss: 0.003581\n",
            "Train Epoch: 4515 \tLoss: 0.003576\n",
            "Train Epoch: 4520 \tLoss: 0.003579\n",
            "Train Epoch: 4525 \tLoss: 0.003568\n",
            "\n",
            "Test set: Avg. loss: 0.1916, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4530 \tLoss: 0.003561\n",
            "Train Epoch: 4535 \tLoss: 0.003556\n",
            "Train Epoch: 4540 \tLoss: 0.003551\n",
            "Train Epoch: 4545 \tLoss: 0.003549\n",
            "Train Epoch: 4550 \tLoss: 0.003542\n",
            "\n",
            "Test set: Avg. loss: 0.1918, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4555 \tLoss: 0.003536\n",
            "Train Epoch: 4560 \tLoss: 0.003535\n",
            "Train Epoch: 4565 \tLoss: 0.003533\n",
            "Train Epoch: 4570 \tLoss: 0.003522\n",
            "Train Epoch: 4575 \tLoss: 0.003517\n",
            "\n",
            "Test set: Avg. loss: 0.1931, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4580 \tLoss: 0.003513\n",
            "Train Epoch: 4585 \tLoss: 0.003507\n",
            "Train Epoch: 4590 \tLoss: 0.003504\n",
            "Train Epoch: 4595 \tLoss: 0.003497\n",
            "Train Epoch: 4600 \tLoss: 0.003494\n",
            "\n",
            "Test set: Avg. loss: 0.1923, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4605 \tLoss: 0.003488\n",
            "Train Epoch: 4610 \tLoss: 0.003486\n",
            "Train Epoch: 4615 \tLoss: 0.003478\n",
            "Train Epoch: 4620 \tLoss: 0.003476\n",
            "Train Epoch: 4625 \tLoss: 0.003468\n",
            "\n",
            "Test set: Avg. loss: 0.1919, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4630 \tLoss: 0.003463\n",
            "Train Epoch: 4635 \tLoss: 0.003459\n",
            "Train Epoch: 4640 \tLoss: 0.003454\n",
            "Train Epoch: 4645 \tLoss: 0.003449\n",
            "Train Epoch: 4650 \tLoss: 0.003445\n",
            "\n",
            "Test set: Avg. loss: 0.1919, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4655 \tLoss: 0.003443\n",
            "Train Epoch: 4660 \tLoss: 0.003435\n",
            "Train Epoch: 4665 \tLoss: 0.003432\n",
            "Train Epoch: 4670 \tLoss: 0.003426\n",
            "Train Epoch: 4675 \tLoss: 0.003423\n",
            "\n",
            "Test set: Avg. loss: 0.1918, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4680 \tLoss: 0.003418\n",
            "Train Epoch: 4685 \tLoss: 0.003412\n",
            "Train Epoch: 4690 \tLoss: 0.003407\n",
            "Train Epoch: 4695 \tLoss: 0.003404\n",
            "Train Epoch: 4700 \tLoss: 0.003399\n",
            "\n",
            "Test set: Avg. loss: 0.1918, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4705 \tLoss: 0.003395\n",
            "Train Epoch: 4710 \tLoss: 0.003389\n",
            "Train Epoch: 4715 \tLoss: 0.003385\n",
            "Train Epoch: 4720 \tLoss: 0.003381\n",
            "Train Epoch: 4725 \tLoss: 0.003377\n",
            "\n",
            "Test set: Avg. loss: 0.1920, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4730 \tLoss: 0.003372\n",
            "Train Epoch: 4735 \tLoss: 0.003369\n",
            "Train Epoch: 4740 \tLoss: 0.003362\n",
            "Train Epoch: 4745 \tLoss: 0.003357\n",
            "Train Epoch: 4750 \tLoss: 0.003354\n",
            "\n",
            "Test set: Avg. loss: 0.1918, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4755 \tLoss: 0.003351\n",
            "Train Epoch: 4760 \tLoss: 0.003345\n",
            "Train Epoch: 4765 \tLoss: 0.003339\n",
            "Train Epoch: 4770 \tLoss: 0.003335\n",
            "Train Epoch: 4775 \tLoss: 0.003331\n",
            "\n",
            "Test set: Avg. loss: 0.1915, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4780 \tLoss: 0.003335\n",
            "Train Epoch: 4785 \tLoss: 0.003326\n",
            "Train Epoch: 4790 \tLoss: 0.003319\n",
            "Train Epoch: 4795 \tLoss: 0.003313\n",
            "Train Epoch: 4800 \tLoss: 0.003309\n",
            "\n",
            "Test set: Avg. loss: 0.1917, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4805 \tLoss: 0.003305\n",
            "Train Epoch: 4810 \tLoss: 0.003307\n",
            "Train Epoch: 4815 \tLoss: 0.003298\n",
            "Train Epoch: 4820 \tLoss: 0.003292\n",
            "Train Epoch: 4825 \tLoss: 0.003288\n",
            "\n",
            "Test set: Avg. loss: 0.1917, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4830 \tLoss: 0.003283\n",
            "Train Epoch: 4835 \tLoss: 0.003284\n",
            "Train Epoch: 4840 \tLoss: 0.003275\n",
            "Train Epoch: 4845 \tLoss: 0.003270\n",
            "Train Epoch: 4850 \tLoss: 0.003266\n",
            "\n",
            "Test set: Avg. loss: 0.1919, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4855 \tLoss: 0.003262\n",
            "Train Epoch: 4860 \tLoss: 0.003260\n",
            "Train Epoch: 4865 \tLoss: 0.003253\n",
            "Train Epoch: 4870 \tLoss: 0.003253\n",
            "Train Epoch: 4875 \tLoss: 0.003245\n",
            "\n",
            "Test set: Avg. loss: 0.1916, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4880 \tLoss: 0.003241\n",
            "Train Epoch: 4885 \tLoss: 0.003237\n",
            "Train Epoch: 4890 \tLoss: 0.003233\n",
            "Train Epoch: 4895 \tLoss: 0.003228\n",
            "Train Epoch: 4900 \tLoss: 0.003224\n",
            "\n",
            "Test set: Avg. loss: 0.1926, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4905 \tLoss: 0.003220\n",
            "Train Epoch: 4910 \tLoss: 0.003216\n",
            "Train Epoch: 4915 \tLoss: 0.003224\n",
            "Train Epoch: 4920 \tLoss: 0.003215\n",
            "Train Epoch: 4925 \tLoss: 0.003208\n",
            "\n",
            "Test set: Avg. loss: 0.1934, Accuracy: 465/500 (93%)\n",
            "\n",
            "Train Epoch: 4930 \tLoss: 0.003201\n",
            "Train Epoch: 4935 \tLoss: 0.003195\n",
            "Train Epoch: 4940 \tLoss: 0.003192\n",
            "Train Epoch: 4945 \tLoss: 0.003189\n",
            "Train Epoch: 4950 \tLoss: 0.003185\n",
            "\n",
            "Test set: Avg. loss: 0.1914, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4955 \tLoss: 0.003179\n",
            "Train Epoch: 4960 \tLoss: 0.003175\n",
            "Train Epoch: 4965 \tLoss: 0.003171\n",
            "Train Epoch: 4970 \tLoss: 0.003167\n",
            "Train Epoch: 4975 \tLoss: 0.003164\n",
            "\n",
            "Test set: Avg. loss: 0.1921, Accuracy: 466/500 (93%)\n",
            "\n",
            "Train Epoch: 4980 \tLoss: 0.003159\n",
            "Train Epoch: 4985 \tLoss: 0.003155\n",
            "Train Epoch: 4990 \tLoss: 0.003152\n",
            "Train Epoch: 4995 \tLoss: 0.003148\n",
            "Train Epoch: 5000 \tLoss: 0.003144\n",
            "\n",
            "Test set: Avg. loss: 0.1910, Accuracy: 466/500 (93%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_n = 25\n",
        "test()\n",
        "count = []\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  if epoch % test_n == 0:\n",
        "    test()\n",
        "    count.append(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.arange(0,n_epochs)\n",
        "count = np.arange(0,n_epochs+test_n,test_n)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(x, train_losses, color='blue', zorder=1)\n",
        "plt.plot(count, test_losses, color='red', zorder=2)\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the final weights to measure later their contribution:\n",
        "conv_1_final= network.conv1.weight.clone()\n",
        "conv_2_final =network.conv2.weight.clone()\n",
        "fc_1_final= network.fc1.weight.clone()\n",
        "fc_2_final= network.fc2.weight.clone()\n",
        "first_losses=test_losses.copy()\n",
        "test_losses.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 1.1091, Accuracy: 331/500 (66%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 1.0625, Accuracy: 449/500 (90%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 5.2736, Accuracy: 50/500 (10%)\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 2.9156, Accuracy: 51/500 (10%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# calculate the contribution of every layer\n",
        "\n",
        "with torch.no_grad():\n",
        "    network.fc1.weight[:] = fc1_init\n",
        "test()\n",
        "\n",
        "with torch.no_grad():\n",
        "    network.fc1.weight[:] = fc_1_final\n",
        "    network.fc2.weight[:] = fc2_init\n",
        "test()\n",
        "\n",
        "with torch.no_grad():\n",
        "    network.fc2.weight[:] = fc_2_final\n",
        "    network.conv1.weight[:] = conv1_init\n",
        "test()\n",
        "\n",
        "with torch.no_grad():\n",
        "    network.conv1.weight[:] = conv_1_final\n",
        "    network.conv2.weight[:] = conv2_init\n",
        "test()\n",
        "\n",
        "with torch.no_grad():\n",
        "    network.conv2.weight[:] = conv_2_final\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fc1 contribution is:  tensor(1.1091)\n",
            "fc2 contribution is:  tensor(1.0625)\n",
            "conv1 contribution is:  tensor(5.2736)\n",
            "conv2 contribution is:  tensor(2.9156)\n"
          ]
        }
      ],
      "source": [
        "print(\"fc1 contribution is: \",test_losses[0])\n",
        "\n",
        "print(\"fc2 contribution is: \",test_losses[1])\n",
        "\n",
        "print(\"conv1 contribution is: \",test_losses[2])\n",
        "\n",
        "print(\"conv2 contribution is: \",test_losses[3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
