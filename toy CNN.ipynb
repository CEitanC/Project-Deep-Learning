{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0f5ad0149476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnew_alg_v2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mna2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import new_alg_v2 as na2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s = 30 # number of sample\n",
    "n_ch = 3 # number of input channels \n",
    "d0 = 10 # dim0 of input image\n",
    "d1 = 10 # dim1 of input image\n",
    "n_epoch = 1\n",
    "output_dim = n_s\n",
    "target = torch.ones(output_dim,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "DS = torch.normal(0, 1, size=(n_s, n_ch, d0, d1))\n",
    "# לעשות את הרשת כללית מבחינת מימדים וכו"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_CNN(nn.Module):\n",
    "    def __init__(self, n_ch):\n",
    "        super(toy_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_ch, 1, kernel_size=5, bias=False)\n",
    "        self.fc1 = nn.Linear(9, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2)\n",
    "        x = x.view(-1, 9)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, eta, n_epoch, dataset, target):\n",
    "    train_losses = []\n",
    "    net.train()\n",
    "    loss = torch.nn.MSELoss()\n",
    "    optimizer = na2.new_alg(net.parameters(), lr=eta)\n",
    "    for epoch in range(n_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        py_hat = net(dataset)\n",
    "        objective = loss(py_hat, target)\n",
    "        train_losses.append(float(objective))\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, dataset, target):\n",
    "    net.eval()\n",
    "    loss = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        output = net(dataset)\n",
    "        test_loss = loss(output, target)\n",
    "    return float(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xl_calc(eta, n_ch):\n",
    "    network = toy_CNN(n_ch)\n",
    "    conv1_init = network.conv1.weight.clone()\n",
    "    fc1_init = network.fc1.weight.clone()\n",
    "    \n",
    "    initial_loss = test(network, DS, target)\n",
    "    test_loss = train(network, eta, n_epoch, DS, target)\n",
    "    final_loss = test(network, DS, target)\n",
    "    \n",
    "    conv1_final = network.conv1.weight.clone()\n",
    "    fc1_final = network.fc1.weight.clone()\n",
    "    \n",
    "    #init weights conv1\n",
    "    with torch.no_grad():\n",
    "      network.conv1.weight[:] = conv1_init\n",
    "    \n",
    "    loss_conv1 = test(network, DS, target)\n",
    "    \n",
    "    #init weights fc1\n",
    "    with torch.no_grad():\n",
    "      network.conv1.weight[:] = conv1_final\n",
    "      network.fc1.weight[:] = fc1_init\n",
    "    \n",
    "    loss_fc1 = test(network, DS, target)\n",
    "    \n",
    "    dL = final_loss - initial_loss\n",
    "    print(\"for lr =\",  eta, \":\")\n",
    "    print(\"  inital loss = \", initial_loss)\n",
    "    print(\"  final loss = \", final_loss)\n",
    "    if dL == 0:\n",
    "      #print(\"for lr =\",  eta, \":\")\n",
    "      print(\"   ***dL zero***\")\n",
    "      return\n",
    "    \n",
    "    Xl1 = (final_loss - loss_conv1)/dL\n",
    "    Xl2 = (final_loss-loss_fc1)/dL\n",
    "    if True:\n",
    "      #print(\"for lr =\",  eta, \":\")\n",
    "      print(\"  Xl1 = \", float(Xl1))\n",
    "      print(\"  Xl2 = \", float(Xl2))\n",
    "    return Xl1, Xl2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for lr = 0.01 :\n",
      "  inital loss =  1.4154834747314453\n",
      "  final loss =  0.9628806710243225\n",
      "  Xl1 =  0.44892620777344583\n",
      "  Xl2 =  0.4722896088255416\n",
      "for lr = 0.004641588833612777 :\n",
      "  inital loss =  1.5622518062591553\n",
      "  final loss =  1.338186264038086\n",
      "  Xl1 =  0.47388750147637954\n",
      "  Xl2 =  0.48928707095879015\n",
      "for lr = 0.0021544346900318843 :\n",
      "  inital loss =  1.8761330842971802\n",
      "  final loss =  1.744376301765442\n",
      "  Xl1 =  0.48086868562577356\n",
      "  Xl2 =  0.48850311602017993\n",
      "for lr = 0.001 :\n",
      "  inital loss =  1.3713326454162598\n",
      "  final loss =  1.3346885442733765\n",
      "  Xl1 =  0.49519019626341526\n",
      "  Xl2 =  0.4951804367698679\n",
      "for lr = 0.0004641588833612782 :\n",
      "  inital loss =  1.0641180276870728\n",
      "  final loss =  1.0497593879699707\n",
      "  Xl1 =  0.48945196722264195\n",
      "  Xl2 =  0.5084392564487874\n",
      "for lr = 0.00021544346900318845 :\n",
      "  inital loss =  0.698697030544281\n",
      "  final loss =  0.6933844685554504\n",
      "  Xl1 =  0.4995960955907102\n",
      "  Xl2 =  0.4999102434646023\n",
      "for lr = 0.0001 :\n",
      "  inital loss =  0.20605911314487457\n",
      "  final loss =  0.20552970468997955\n",
      "  Xl1 =  0.49991555955865796\n",
      "  Xl2 =  0.4999437063724386\n",
      "for lr = 4.641588833612782e-05 :\n",
      "  inital loss =  2.6863529682159424\n",
      "  final loss =  2.6807093620300293\n",
      "  Xl1 =  0.49959866503316297\n",
      "  Xl2 =  0.4995564192471801\n",
      "for lr = 2.1544346900318867e-05 :\n",
      "  inital loss =  1.3181045055389404\n",
      "  final loss =  1.3173775672912598\n",
      "  Xl1 =  0.5001639881928501\n",
      "  Xl2 =  0.5001639881928501\n",
      "for lr = 1e-05 :\n",
      "  inital loss =  0.13279825448989868\n",
      "  final loss =  0.13275285065174103\n",
      "  Xl1 =  0.49983590416803414\n",
      "  Xl2 =  0.49983590416803414\n"
     ]
    }
   ],
   "source": [
    "lrs = np.logspace(-2,-5,10)\n",
    "for lr in lrs:\n",
    "    xl_calc(lr, n_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for lr = 0.01 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.2367421388626099\n",
      "  Xl1 =  0.404560171575731\n",
      "  Xl2 =  0.45579965165931063\n",
      "for lr = 0.004641588833612777 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.4420363903045654\n",
      "  Xl1 =  0.4608490196274432\n",
      "  Xl2 =  0.4747587408595628\n",
      "for lr = 0.0021544346900318843 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.557746410369873\n",
      "  Xl1 =  0.483146635186553\n",
      "  Xl2 =  0.48736333620395844\n",
      "for lr = 0.001 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.6159354448318481\n",
      "  Xl1 =  0.49267850112899503\n",
      "  Xl2 =  0.49371473304584307\n",
      "for lr = 0.0004641588833612782 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.6439149379730225\n",
      "  Xl1 =  0.4967459004134564\n",
      "  Xl2 =  0.4969581767393391\n",
      "for lr = 0.00021544346900318845 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.6571029424667358\n",
      "  Xl1 =  0.4984893634632895\n",
      "  Xl2 =  0.49861352537041637\n",
      "for lr = 0.0001 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.663265347480774\n",
      "  Xl1 =  0.49931040619299727\n",
      "  Xl2 =  0.49933265115451353\n",
      "for lr = 4.641588833612782e-05 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.6661343574523926\n",
      "  Xl1 =  0.49959304830755974\n",
      "  Xl2 =  0.49959304830755974\n",
      "for lr = 2.1544346900318867e-05 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.6674683094024658\n",
      "  Xl1 =  0.49963906362792615\n",
      "  Xl2 =  0.49953593895019077\n",
      "for lr = 1e-05 :\n",
      "  inital loss =  1.6686242818832397\n",
      "  final loss =  1.6680872440338135\n",
      "  Xl1 =  0.500110987791343\n",
      "  Xl2 =  0.49988901220865706\n"
     ]
    }
   ],
   "source": [
    "network = toy_CNN(n_ch)\n",
    "conv1_init = network.conv1.weight.clone()\n",
    "fc1_init = network.fc1.weight.clone()\n",
    "\n",
    "def xl_calc_db(eta):\n",
    "    \n",
    "    #init weights conv1\n",
    "    with torch.no_grad():\n",
    "      network.conv1.weight[:] = conv1_init\n",
    "      network.fc1.weight[:] = fc1_init\n",
    "\n",
    "    initial_loss = test(network, DS, target)\n",
    "    test_loss = train(network, eta, n_epoch, DS, target)\n",
    "    final_loss = test(network, DS, target)\n",
    "  \n",
    "    conv1_final = network.conv1.weight.clone()\n",
    "    fc1_final = network.fc1.weight.clone()\n",
    "    \n",
    "    #init weights conv1\n",
    "    with torch.no_grad():\n",
    "      network.conv1.weight[:] = conv1_init\n",
    "    \n",
    "    loss_conv1 = test(network, DS, target)\n",
    "    \n",
    "    #init weights fc1\n",
    "    with torch.no_grad():\n",
    "      network.conv1.weight[:] = conv1_final\n",
    "      network.fc1.weight[:] = fc1_init\n",
    "    \n",
    "    loss_fc1 = test(network, DS, target)\n",
    "    \n",
    "    dL = final_loss - initial_loss\n",
    "    print(\"for lr =\",  eta, \":\")\n",
    "    print(\"  inital loss = \", initial_loss)\n",
    "    print(\"  final loss = \", final_loss)\n",
    "    if dL == 0:\n",
    "      #print(\"for lr =\",  eta, \":\")\n",
    "      print(\"   ***dL zero***\")\n",
    "      return\n",
    "    \n",
    "    Xl1 = (final_loss - loss_conv1)/dL\n",
    "    Xl2 = (final_loss-loss_fc1)/dL\n",
    "    if True:\n",
    "      #print(\"for lr =\",  eta, \":\")\n",
    "      print(\"  Xl1 = \", float(Xl1))\n",
    "      print(\"  Xl2 = \", float(Xl2))\n",
    "    return Xl1, Xl2\n",
    "    \n",
    "lrs = np.logspace(-2,-5,10)\n",
    "for lr in lrs:\n",
    "    xl_calc_db(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital loss =  1.6914465427398682\n",
      "final loss =  1.5899059772491455\n",
      "Xl1 =  0.4827315375729058\n",
      "Xl2 =  0.4968759685554084\n"
     ]
    }
   ],
   "source": [
    "eta = 0.0021544346900318843\n",
    "network = toy_CNN(n_ch)\n",
    "\n",
    "conv1_init = network.conv1.weight.clone()\n",
    "fc1_init = network.fc1.weight.clone()\n",
    "\n",
    "initial_loss = test(network, DS, target)\n",
    "train_loss = train(network, eta, n_epoch, DS, target)\n",
    "final_loss = test(network, DS, target)\n",
    "\n",
    "conv1_final = network.conv1.weight.clone()\n",
    "fc1_final = network.fc1.weight.clone()\n",
    "\n",
    "#init weights conv1\n",
    "with torch.no_grad():\n",
    "  network.conv1.weight[:] = conv1_init\n",
    "\n",
    "loss_conv1 = test(network, DS, target)\n",
    "\n",
    "#init weights conv1\n",
    "with torch.no_grad():\n",
    "  network.conv1.weight[:] = conv1_final\n",
    "  network.fc1.weight[:] = fc1_init\n",
    "\n",
    "loss_fc1 = test(network, DS, target)\n",
    "\n",
    "dL = final_loss - initial_loss\n",
    "Xl1 = (final_loss - loss_conv1)/dL\n",
    "Xl2 = (final_loss-loss_fc1)/dL\n",
    "print(\"inital loss = \", initial_loss)\n",
    "print(\"final loss = \", final_loss)\n",
    "print(\"Xl1 = \", float(Xl1))\n",
    "print(\"Xl2 = \", float(Xl2))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7894602a0c985887175ef870b163991377e40017ef89bcf79ed228593e61d67"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
