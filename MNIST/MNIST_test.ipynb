{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-1xNKXx6TBQp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "# MNIST dataset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "    \n",
        "def define_dataloaders(n_examples_train, n_examples_test, classes=np.arange(10), zscore_images=True):\n",
        "    # MNIST data, batch training\n",
        "    batch_size = n_examples_train\n",
        "    batches_per_epoch_train = n_examples_train / batch_size\n",
        "    batches_per_epoch_test = n_examples_test / batch_size\n",
        "\n",
        "    # Choose the classes (at most 10)\n",
        "    assert max(classes) <= 9\n",
        "\n",
        "    # Transformation for the images\n",
        "    transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "    #transform = transforms.Compose([transforms.ToTensor(),\n",
        "    #                              transforms.Normalize((0.5,), (0.5,)),\n",
        "    #                              ])\n",
        "    trainset = MNIST(data_dir, download=True, train=True, transform=transform)\n",
        "    testset = MNIST(data_dir, download=True, train=False, transform=transform)\n",
        "\n",
        "    # Obtain training and test data. \n",
        "    # Note that both datasets are sorted, but the train and test loaders will shuffle them during training.\n",
        "    n_examples_tt = [n_examples_train, n_examples_test]\n",
        "    for i_d, (n_examples_i, dataset) in enumerate(zip(n_examples_tt, [trainset, testset])):\n",
        "        n_per_class = n_examples_i // len(classes)\n",
        "        data_orig = dataset.data.detach().clone()\n",
        "        targets_orig = dataset.targets.detach().clone()\n",
        "        for i_c, class_i in enumerate(classes):\n",
        "            mask = targets_orig == class_i\n",
        "            i0 = i_c * n_per_class\n",
        "            i1 = (i_c+1) * n_per_class\n",
        "            dataset.data[i0:i1] = data_orig[mask][:n_per_class]\n",
        "            dataset.targets[i0:i1] = targets_orig[mask][:n_per_class]\n",
        "        # Fill the remaining slots with random classes from the available choices\n",
        "        n_remain = n_examples_i - i1 \n",
        "        for i in range(n_remain):\n",
        "            class_i = np.random.choice(classes)\n",
        "            mask = targets_orig == class_i\n",
        "            idx_i = np.random.choice(torch.where(mask)[0][i1:].cpu())\n",
        "            dataset.data[i1+i] = data_orig[idx_i]\n",
        "            dataset.targets[i1+i] = targets_orig[idx_i]\n",
        "\n",
        "        # Cut off\n",
        "        dataset.data = dataset.data[:n_examples_i]\n",
        "        dataset.targets = dataset.targets[:n_examples_i]\n",
        "\n",
        "    # Batch-loader\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rRSXyp6THwR",
        "outputId": "edd158a8-0f57-48d7-be07-55ffef562fac"
      },
      "outputs": [],
      "source": [
        "n_epochs = 2500\n",
        "learning_rate = 0.01\n",
        "momentum = 0\n",
        "#torch.backends.cudnn.enabled = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_HKx24x7V_4d"
      },
      "outputs": [],
      "source": [
        "## no dopout\n",
        "#Building the Network\n",
        "\n",
        "conv1_out= 10\n",
        "conv2_out=20\n",
        "fc1_in=320\n",
        "fc2_in=50\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=5)\n",
        "        #self.conv2_drop = nn.Dropout2d() --- no dropout\n",
        "        self.fc1 = nn.Linear(fc1_in, fc2_in)\n",
        "        self.fc2 = nn.Linear(fc2_in, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(F.max_pool2d(self.conv1(x), 2))\n",
        "        #x = torch.tanh(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = torch.tanh(F.max_pool2d((self.conv2(x)), 2))\n",
        "        x = x.view(-1, fc1_in)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "      #  x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "taugfbRTWCo-"
      },
      "outputs": [],
      "source": [
        "network = Net()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
        "loss_f=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the initial weights to measure later their contribution:\n",
        "fc1_init = network.fc1.weight.clone()\n",
        "fc2_init = network.fc2.weight.clone()\n",
        "conv1_init = network.conv1.weight.clone()\n",
        "conv2_init = network.conv2.weight.clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XKYJb4e7WLNG"
      },
      "outputs": [],
      "source": [
        "data_dir = '/files/'\n",
        "n_examples_train = 1000\n",
        "n_examples_test = 500\n",
        "train_loader, test_loader = define_dataloaders(n_examples_train, n_examples_test)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y2MhJ3U3WNTW"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "  train_n = 5\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = loss_f(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % train_n == 0:\n",
        "      print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
        "          epoch,\n",
        "          loss.item()))\n",
        "    train_losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r9W0MBhnWPfg"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += loss_f(output,target)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= 1 #1 is the amount of test batches\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss,\n",
        "    correct,\n",
        "    len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "BJo6gYxIWSUC",
        "outputId": "9cd112a3-872f-4d2c-c351-b99e8929a6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.2958, Accuracy: 61/500 (12%)\n",
            "\n",
            "Train Epoch: 5 \tLoss: 2.287567\n",
            "Train Epoch: 10 \tLoss: 2.277916\n",
            "Train Epoch: 15 \tLoss: 2.268316\n",
            "Train Epoch: 20 \tLoss: 2.258716\n",
            "Train Epoch: 25 \tLoss: 2.249041\n",
            "\n",
            "Test set: Avg. loss: 2.2528, Accuracy: 118/500 (24%)\n",
            "\n",
            "Train Epoch: 30 \tLoss: 2.239226\n",
            "Train Epoch: 35 \tLoss: 2.229207\n",
            "Train Epoch: 40 \tLoss: 2.218948\n",
            "Train Epoch: 45 \tLoss: 2.208387\n",
            "Train Epoch: 50 \tLoss: 2.197458\n",
            "\n",
            "Test set: Avg. loss: 2.2058, Accuracy: 173/500 (35%)\n",
            "\n",
            "Train Epoch: 55 \tLoss: 2.186148\n",
            "Train Epoch: 60 \tLoss: 2.174417\n",
            "Train Epoch: 65 \tLoss: 2.162232\n",
            "Train Epoch: 70 \tLoss: 2.149568\n",
            "Train Epoch: 75 \tLoss: 2.136359\n",
            "\n",
            "Test set: Avg. loss: 2.1492, Accuracy: 256/500 (51%)\n",
            "\n",
            "Train Epoch: 80 \tLoss: 2.122597\n",
            "Train Epoch: 85 \tLoss: 2.108259\n",
            "Train Epoch: 90 \tLoss: 2.093316\n",
            "Train Epoch: 95 \tLoss: 2.077749\n",
            "Train Epoch: 100 \tLoss: 2.061566\n",
            "\n",
            "Test set: Avg. loss: 2.0791, Accuracy: 285/500 (57%)\n",
            "\n",
            "Train Epoch: 105 \tLoss: 2.044761\n",
            "Train Epoch: 110 \tLoss: 2.027328\n",
            "Train Epoch: 115 \tLoss: 2.009273\n",
            "Train Epoch: 120 \tLoss: 1.990611\n",
            "Train Epoch: 125 \tLoss: 1.971388\n",
            "\n",
            "Test set: Avg. loss: 1.9939, Accuracy: 294/500 (59%)\n",
            "\n",
            "Train Epoch: 130 \tLoss: 1.951599\n",
            "Train Epoch: 135 \tLoss: 1.931278\n",
            "Train Epoch: 140 \tLoss: 1.910466\n",
            "Train Epoch: 145 \tLoss: 1.889199\n",
            "Train Epoch: 150 \tLoss: 1.867531\n",
            "\n",
            "Test set: Avg. loss: 1.8955, Accuracy: 297/500 (59%)\n",
            "\n",
            "Train Epoch: 155 \tLoss: 1.845519\n",
            "Train Epoch: 160 \tLoss: 1.823208\n",
            "Train Epoch: 165 \tLoss: 1.800619\n",
            "Train Epoch: 170 \tLoss: 1.777812\n",
            "Train Epoch: 175 \tLoss: 1.754850\n",
            "\n",
            "Test set: Avg. loss: 1.7894, Accuracy: 301/500 (60%)\n",
            "\n",
            "Train Epoch: 180 \tLoss: 1.731790\n",
            "Train Epoch: 185 \tLoss: 1.708655\n",
            "Train Epoch: 190 \tLoss: 1.685494\n",
            "Train Epoch: 195 \tLoss: 1.662343\n",
            "Train Epoch: 200 \tLoss: 1.639251\n",
            "\n",
            "Test set: Avg. loss: 1.6817, Accuracy: 308/500 (62%)\n",
            "\n",
            "Train Epoch: 205 \tLoss: 1.616270\n",
            "Train Epoch: 210 \tLoss: 1.593415\n",
            "Train Epoch: 215 \tLoss: 1.570722\n",
            "Train Epoch: 220 \tLoss: 1.548214\n",
            "Train Epoch: 225 \tLoss: 1.525914\n",
            "\n",
            "Test set: Avg. loss: 1.5770, Accuracy: 319/500 (64%)\n",
            "\n",
            "Train Epoch: 230 \tLoss: 1.503845\n",
            "Train Epoch: 235 \tLoss: 1.482017\n",
            "Train Epoch: 240 \tLoss: 1.460453\n",
            "Train Epoch: 245 \tLoss: 1.439183\n",
            "Train Epoch: 250 \tLoss: 1.418215\n",
            "\n",
            "Test set: Avg. loss: 1.4784, Accuracy: 327/500 (65%)\n",
            "\n",
            "Train Epoch: 255 \tLoss: 1.397565\n",
            "Train Epoch: 260 \tLoss: 1.377244\n",
            "Train Epoch: 265 \tLoss: 1.357271\n",
            "Train Epoch: 270 \tLoss: 1.337648\n",
            "Train Epoch: 275 \tLoss: 1.318379\n",
            "\n",
            "Test set: Avg. loss: 1.3874, Accuracy: 331/500 (66%)\n",
            "\n",
            "Train Epoch: 280 \tLoss: 1.299473\n",
            "Train Epoch: 285 \tLoss: 1.280933\n",
            "Train Epoch: 290 \tLoss: 1.262770\n",
            "Train Epoch: 295 \tLoss: 1.244973\n",
            "Train Epoch: 300 \tLoss: 1.227542\n",
            "\n",
            "Test set: Avg. loss: 1.3048, Accuracy: 340/500 (68%)\n",
            "\n",
            "Train Epoch: 305 \tLoss: 1.210482\n",
            "Train Epoch: 310 \tLoss: 1.193799\n",
            "Train Epoch: 315 \tLoss: 1.177488\n",
            "Train Epoch: 320 \tLoss: 1.161544\n",
            "Train Epoch: 325 \tLoss: 1.145951\n",
            "\n",
            "Test set: Avg. loss: 1.2305, Accuracy: 352/500 (70%)\n",
            "\n",
            "Train Epoch: 330 \tLoss: 1.130717\n",
            "Train Epoch: 335 \tLoss: 1.115839\n",
            "Train Epoch: 340 \tLoss: 1.101308\n",
            "Train Epoch: 345 \tLoss: 1.087120\n",
            "Train Epoch: 350 \tLoss: 1.073269\n",
            "\n",
            "Test set: Avg. loss: 1.1643, Accuracy: 357/500 (71%)\n",
            "\n",
            "Train Epoch: 355 \tLoss: 1.059743\n",
            "Train Epoch: 360 \tLoss: 1.046537\n",
            "Train Epoch: 365 \tLoss: 1.033641\n",
            "Train Epoch: 370 \tLoss: 1.021050\n",
            "Train Epoch: 375 \tLoss: 1.008764\n",
            "\n",
            "Test set: Avg. loss: 1.1054, Accuracy: 361/500 (72%)\n",
            "\n",
            "Train Epoch: 380 \tLoss: 0.996764\n",
            "Train Epoch: 385 \tLoss: 0.985046\n",
            "Train Epoch: 390 \tLoss: 0.973601\n",
            "Train Epoch: 395 \tLoss: 0.962421\n",
            "Train Epoch: 400 \tLoss: 0.951498\n",
            "\n",
            "Test set: Avg. loss: 1.0532, Accuracy: 368/500 (74%)\n",
            "\n",
            "Train Epoch: 405 \tLoss: 0.940826\n",
            "Train Epoch: 410 \tLoss: 0.930400\n",
            "Train Epoch: 415 \tLoss: 0.920210\n",
            "Train Epoch: 420 \tLoss: 0.910247\n",
            "Train Epoch: 425 \tLoss: 0.900505\n",
            "\n",
            "Test set: Avg. loss: 1.0068, Accuracy: 374/500 (75%)\n",
            "\n",
            "Train Epoch: 430 \tLoss: 0.890976\n",
            "Train Epoch: 435 \tLoss: 0.881653\n",
            "Train Epoch: 440 \tLoss: 0.872525\n",
            "Train Epoch: 445 \tLoss: 0.863591\n",
            "Train Epoch: 450 \tLoss: 0.854846\n",
            "\n",
            "Test set: Avg. loss: 0.9653, Accuracy: 378/500 (76%)\n",
            "\n",
            "Train Epoch: 455 \tLoss: 0.846283\n",
            "Train Epoch: 460 \tLoss: 0.837895\n",
            "Train Epoch: 465 \tLoss: 0.829678\n",
            "Train Epoch: 470 \tLoss: 0.821624\n",
            "Train Epoch: 475 \tLoss: 0.813729\n",
            "\n",
            "Test set: Avg. loss: 0.9281, Accuracy: 381/500 (76%)\n",
            "\n",
            "Train Epoch: 480 \tLoss: 0.805985\n",
            "Train Epoch: 485 \tLoss: 0.798392\n",
            "Train Epoch: 490 \tLoss: 0.790943\n",
            "Train Epoch: 495 \tLoss: 0.783633\n",
            "Train Epoch: 500 \tLoss: 0.776460\n",
            "\n",
            "Test set: Avg. loss: 0.8946, Accuracy: 383/500 (77%)\n",
            "\n",
            "Train Epoch: 505 \tLoss: 0.769420\n",
            "Train Epoch: 510 \tLoss: 0.762509\n",
            "Train Epoch: 515 \tLoss: 0.755723\n",
            "Train Epoch: 520 \tLoss: 0.749056\n",
            "Train Epoch: 525 \tLoss: 0.742508\n",
            "\n",
            "Test set: Avg. loss: 0.8641, Accuracy: 388/500 (78%)\n",
            "\n",
            "Train Epoch: 530 \tLoss: 0.736074\n",
            "Train Epoch: 535 \tLoss: 0.729748\n",
            "Train Epoch: 540 \tLoss: 0.723530\n",
            "Train Epoch: 545 \tLoss: 0.717418\n",
            "Train Epoch: 550 \tLoss: 0.711409\n",
            "\n",
            "Test set: Avg. loss: 0.8363, Accuracy: 392/500 (78%)\n",
            "\n",
            "Train Epoch: 555 \tLoss: 0.705501\n",
            "Train Epoch: 560 \tLoss: 0.699690\n",
            "Train Epoch: 565 \tLoss: 0.693973\n",
            "Train Epoch: 570 \tLoss: 0.688350\n",
            "Train Epoch: 575 \tLoss: 0.682818\n",
            "\n",
            "Test set: Avg. loss: 0.8108, Accuracy: 393/500 (79%)\n",
            "\n",
            "Train Epoch: 580 \tLoss: 0.677375\n",
            "Train Epoch: 585 \tLoss: 0.672014\n",
            "Train Epoch: 590 \tLoss: 0.666733\n",
            "Train Epoch: 595 \tLoss: 0.661536\n",
            "Train Epoch: 600 \tLoss: 0.656420\n",
            "\n",
            "Test set: Avg. loss: 0.7873, Accuracy: 395/500 (79%)\n",
            "\n",
            "Train Epoch: 605 \tLoss: 0.651383\n",
            "Train Epoch: 610 \tLoss: 0.646420\n",
            "Train Epoch: 615 \tLoss: 0.641530\n",
            "Train Epoch: 620 \tLoss: 0.636710\n",
            "Train Epoch: 625 \tLoss: 0.631961\n",
            "\n",
            "Test set: Avg. loss: 0.7655, Accuracy: 395/500 (79%)\n",
            "\n",
            "Train Epoch: 630 \tLoss: 0.627282\n",
            "Train Epoch: 635 \tLoss: 0.622670\n",
            "Train Epoch: 640 \tLoss: 0.618125\n",
            "Train Epoch: 645 \tLoss: 0.613644\n",
            "Train Epoch: 650 \tLoss: 0.609225\n",
            "\n",
            "Test set: Avg. loss: 0.7452, Accuracy: 396/500 (79%)\n",
            "\n",
            "Train Epoch: 655 \tLoss: 0.604868\n",
            "Train Epoch: 660 \tLoss: 0.600571\n",
            "Train Epoch: 665 \tLoss: 0.596334\n",
            "Train Epoch: 670 \tLoss: 0.592155\n",
            "Train Epoch: 675 \tLoss: 0.588030\n",
            "\n",
            "Test set: Avg. loss: 0.7263, Accuracy: 398/500 (80%)\n",
            "\n",
            "Train Epoch: 680 \tLoss: 0.583959\n",
            "Train Epoch: 685 \tLoss: 0.579943\n",
            "Train Epoch: 690 \tLoss: 0.575981\n",
            "Train Epoch: 695 \tLoss: 0.572069\n",
            "Train Epoch: 700 \tLoss: 0.568208\n",
            "\n",
            "Test set: Avg. loss: 0.7085, Accuracy: 401/500 (80%)\n",
            "\n",
            "Train Epoch: 705 \tLoss: 0.564398\n",
            "Train Epoch: 710 \tLoss: 0.560636\n",
            "Train Epoch: 715 \tLoss: 0.556921\n",
            "Train Epoch: 720 \tLoss: 0.553252\n",
            "Train Epoch: 725 \tLoss: 0.549628\n",
            "\n",
            "Test set: Avg. loss: 0.6919, Accuracy: 404/500 (81%)\n",
            "\n",
            "Train Epoch: 730 \tLoss: 0.546048\n",
            "Train Epoch: 735 \tLoss: 0.542513\n",
            "Train Epoch: 740 \tLoss: 0.539020\n",
            "Train Epoch: 745 \tLoss: 0.535570\n",
            "Train Epoch: 750 \tLoss: 0.532160\n",
            "\n",
            "Test set: Avg. loss: 0.6762, Accuracy: 405/500 (81%)\n",
            "\n",
            "Train Epoch: 755 \tLoss: 0.528790\n",
            "Train Epoch: 760 \tLoss: 0.525459\n",
            "Train Epoch: 765 \tLoss: 0.522166\n",
            "Train Epoch: 770 \tLoss: 0.518913\n",
            "Train Epoch: 775 \tLoss: 0.515697\n",
            "\n",
            "Test set: Avg. loss: 0.6614, Accuracy: 405/500 (81%)\n",
            "\n",
            "Train Epoch: 780 \tLoss: 0.512518\n",
            "Train Epoch: 785 \tLoss: 0.509374\n",
            "Train Epoch: 790 \tLoss: 0.506268\n",
            "Train Epoch: 795 \tLoss: 0.503196\n",
            "Train Epoch: 800 \tLoss: 0.500158\n",
            "\n",
            "Test set: Avg. loss: 0.6475, Accuracy: 410/500 (82%)\n",
            "\n",
            "Train Epoch: 805 \tLoss: 0.497154\n",
            "Train Epoch: 810 \tLoss: 0.494182\n",
            "Train Epoch: 815 \tLoss: 0.491242\n",
            "Train Epoch: 820 \tLoss: 0.488333\n",
            "Train Epoch: 825 \tLoss: 0.485456\n",
            "\n",
            "Test set: Avg. loss: 0.6342, Accuracy: 412/500 (82%)\n",
            "\n",
            "Train Epoch: 830 \tLoss: 0.482610\n",
            "Train Epoch: 835 \tLoss: 0.479796\n",
            "Train Epoch: 840 \tLoss: 0.477012\n",
            "Train Epoch: 845 \tLoss: 0.474259\n",
            "Train Epoch: 850 \tLoss: 0.471536\n",
            "\n",
            "Test set: Avg. loss: 0.6216, Accuracy: 414/500 (83%)\n",
            "\n",
            "Train Epoch: 855 \tLoss: 0.468841\n",
            "Train Epoch: 860 \tLoss: 0.466174\n",
            "Train Epoch: 865 \tLoss: 0.463536\n",
            "Train Epoch: 870 \tLoss: 0.460924\n",
            "Train Epoch: 875 \tLoss: 0.458338\n",
            "\n",
            "Test set: Avg. loss: 0.6096, Accuracy: 414/500 (83%)\n",
            "\n",
            "Train Epoch: 880 \tLoss: 0.455780\n",
            "Train Epoch: 885 \tLoss: 0.453248\n",
            "Train Epoch: 890 \tLoss: 0.450742\n",
            "Train Epoch: 895 \tLoss: 0.448260\n",
            "Train Epoch: 900 \tLoss: 0.445804\n",
            "\n",
            "Test set: Avg. loss: 0.5982, Accuracy: 414/500 (83%)\n",
            "\n",
            "Train Epoch: 905 \tLoss: 0.443372\n",
            "Train Epoch: 910 \tLoss: 0.440963\n",
            "Train Epoch: 915 \tLoss: 0.438579\n",
            "Train Epoch: 920 \tLoss: 0.436218\n",
            "Train Epoch: 925 \tLoss: 0.433881\n",
            "\n",
            "Test set: Avg. loss: 0.5873, Accuracy: 415/500 (83%)\n",
            "\n",
            "Train Epoch: 930 \tLoss: 0.431566\n",
            "Train Epoch: 935 \tLoss: 0.429274\n",
            "Train Epoch: 940 \tLoss: 0.427005\n",
            "Train Epoch: 945 \tLoss: 0.424759\n",
            "Train Epoch: 950 \tLoss: 0.422535\n",
            "\n",
            "Test set: Avg. loss: 0.5769, Accuracy: 415/500 (83%)\n",
            "\n",
            "Train Epoch: 955 \tLoss: 0.420334\n",
            "Train Epoch: 960 \tLoss: 0.418155\n",
            "Train Epoch: 965 \tLoss: 0.415996\n",
            "Train Epoch: 970 \tLoss: 0.413857\n",
            "Train Epoch: 975 \tLoss: 0.411739\n",
            "\n",
            "Test set: Avg. loss: 0.5670, Accuracy: 416/500 (83%)\n",
            "\n",
            "Train Epoch: 980 \tLoss: 0.409638\n",
            "Train Epoch: 985 \tLoss: 0.407556\n",
            "Train Epoch: 990 \tLoss: 0.405492\n",
            "Train Epoch: 995 \tLoss: 0.403446\n",
            "Train Epoch: 1000 \tLoss: 0.401418\n",
            "\n",
            "Test set: Avg. loss: 0.5575, Accuracy: 414/500 (83%)\n",
            "\n",
            "Train Epoch: 1005 \tLoss: 0.399408\n",
            "Train Epoch: 1010 \tLoss: 0.397415\n",
            "Train Epoch: 1015 \tLoss: 0.395440\n",
            "Train Epoch: 1020 \tLoss: 0.393483\n",
            "Train Epoch: 1025 \tLoss: 0.391543\n",
            "\n",
            "Test set: Avg. loss: 0.5483, Accuracy: 416/500 (83%)\n",
            "\n",
            "Train Epoch: 1030 \tLoss: 0.389621\n",
            "Train Epoch: 1035 \tLoss: 0.387713\n",
            "Train Epoch: 1040 \tLoss: 0.385823\n",
            "Train Epoch: 1045 \tLoss: 0.383951\n",
            "Train Epoch: 1050 \tLoss: 0.382095\n",
            "\n",
            "Test set: Avg. loss: 0.5396, Accuracy: 418/500 (84%)\n",
            "\n",
            "Train Epoch: 1055 \tLoss: 0.380255\n",
            "Train Epoch: 1060 \tLoss: 0.378430\n",
            "Train Epoch: 1065 \tLoss: 0.376623\n",
            "Train Epoch: 1070 \tLoss: 0.374831\n",
            "Train Epoch: 1075 \tLoss: 0.373054\n",
            "\n",
            "Test set: Avg. loss: 0.5311, Accuracy: 417/500 (83%)\n",
            "\n",
            "Train Epoch: 1080 \tLoss: 0.371292\n",
            "Train Epoch: 1085 \tLoss: 0.369546\n",
            "Train Epoch: 1090 \tLoss: 0.367814\n",
            "Train Epoch: 1095 \tLoss: 0.366098\n",
            "Train Epoch: 1100 \tLoss: 0.364395\n",
            "\n",
            "Test set: Avg. loss: 0.5231, Accuracy: 419/500 (84%)\n",
            "\n",
            "Train Epoch: 1105 \tLoss: 0.362707\n",
            "Train Epoch: 1110 \tLoss: 0.361032\n",
            "Train Epoch: 1115 \tLoss: 0.359369\n",
            "Train Epoch: 1120 \tLoss: 0.357719\n",
            "Train Epoch: 1125 \tLoss: 0.356083\n",
            "\n",
            "Test set: Avg. loss: 0.5153, Accuracy: 420/500 (84%)\n",
            "\n",
            "Train Epoch: 1130 \tLoss: 0.354459\n",
            "Train Epoch: 1135 \tLoss: 0.352847\n",
            "Train Epoch: 1140 \tLoss: 0.351248\n",
            "Train Epoch: 1145 \tLoss: 0.349661\n",
            "Train Epoch: 1150 \tLoss: 0.348085\n",
            "\n",
            "Test set: Avg. loss: 0.5078, Accuracy: 421/500 (84%)\n",
            "\n",
            "Train Epoch: 1155 \tLoss: 0.346522\n",
            "Train Epoch: 1160 \tLoss: 0.344970\n",
            "Train Epoch: 1165 \tLoss: 0.343430\n",
            "Train Epoch: 1170 \tLoss: 0.341900\n",
            "Train Epoch: 1175 \tLoss: 0.340382\n",
            "\n",
            "Test set: Avg. loss: 0.5006, Accuracy: 422/500 (84%)\n",
            "\n",
            "Train Epoch: 1180 \tLoss: 0.338875\n",
            "Train Epoch: 1185 \tLoss: 0.337381\n",
            "Train Epoch: 1190 \tLoss: 0.335899\n",
            "Train Epoch: 1195 \tLoss: 0.334427\n",
            "Train Epoch: 1200 \tLoss: 0.332967\n",
            "\n",
            "Test set: Avg. loss: 0.4936, Accuracy: 423/500 (85%)\n",
            "\n",
            "Train Epoch: 1205 \tLoss: 0.331517\n",
            "Train Epoch: 1210 \tLoss: 0.330078\n",
            "Train Epoch: 1215 \tLoss: 0.328648\n",
            "Train Epoch: 1220 \tLoss: 0.327228\n",
            "Train Epoch: 1225 \tLoss: 0.325817\n",
            "\n",
            "Test set: Avg. loss: 0.4869, Accuracy: 423/500 (85%)\n",
            "\n",
            "Train Epoch: 1230 \tLoss: 0.324416\n",
            "Train Epoch: 1235 \tLoss: 0.323025\n",
            "Train Epoch: 1240 \tLoss: 0.321644\n",
            "Train Epoch: 1245 \tLoss: 0.320273\n",
            "Train Epoch: 1250 \tLoss: 0.318911\n",
            "\n",
            "Test set: Avg. loss: 0.4803, Accuracy: 426/500 (85%)\n",
            "\n",
            "Train Epoch: 1255 \tLoss: 0.317560\n",
            "Train Epoch: 1260 \tLoss: 0.316218\n",
            "Train Epoch: 1265 \tLoss: 0.314885\n",
            "Train Epoch: 1270 \tLoss: 0.313562\n",
            "Train Epoch: 1275 \tLoss: 0.312248\n",
            "\n",
            "Test set: Avg. loss: 0.4740, Accuracy: 426/500 (85%)\n",
            "\n",
            "Train Epoch: 1280 \tLoss: 0.310944\n",
            "Train Epoch: 1285 \tLoss: 0.309649\n",
            "Train Epoch: 1290 \tLoss: 0.308363\n",
            "Train Epoch: 1295 \tLoss: 0.307085\n",
            "Train Epoch: 1300 \tLoss: 0.305816\n",
            "\n",
            "Test set: Avg. loss: 0.4679, Accuracy: 429/500 (86%)\n",
            "\n",
            "Train Epoch: 1305 \tLoss: 0.304556\n",
            "Train Epoch: 1310 \tLoss: 0.303304\n",
            "Train Epoch: 1315 \tLoss: 0.302059\n",
            "Train Epoch: 1320 \tLoss: 0.300822\n",
            "Train Epoch: 1325 \tLoss: 0.299593\n",
            "\n",
            "Test set: Avg. loss: 0.4621, Accuracy: 429/500 (86%)\n",
            "\n",
            "Train Epoch: 1330 \tLoss: 0.298372\n",
            "Train Epoch: 1335 \tLoss: 0.297159\n",
            "Train Epoch: 1340 \tLoss: 0.295953\n",
            "Train Epoch: 1345 \tLoss: 0.294755\n",
            "Train Epoch: 1350 \tLoss: 0.293566\n",
            "\n",
            "Test set: Avg. loss: 0.4563, Accuracy: 429/500 (86%)\n",
            "\n",
            "Train Epoch: 1355 \tLoss: 0.292384\n",
            "Train Epoch: 1360 \tLoss: 0.291209\n",
            "Train Epoch: 1365 \tLoss: 0.290042\n",
            "Train Epoch: 1370 \tLoss: 0.288882\n",
            "Train Epoch: 1375 \tLoss: 0.287729\n",
            "\n",
            "Test set: Avg. loss: 0.4508, Accuracy: 431/500 (86%)\n",
            "\n",
            "Train Epoch: 1380 \tLoss: 0.286582\n",
            "Train Epoch: 1385 \tLoss: 0.285442\n",
            "Train Epoch: 1390 \tLoss: 0.284309\n",
            "Train Epoch: 1395 \tLoss: 0.283184\n",
            "Train Epoch: 1400 \tLoss: 0.282066\n",
            "\n",
            "Test set: Avg. loss: 0.4454, Accuracy: 431/500 (86%)\n",
            "\n",
            "Train Epoch: 1405 \tLoss: 0.280956\n",
            "Train Epoch: 1410 \tLoss: 0.279853\n",
            "Train Epoch: 1415 \tLoss: 0.278757\n",
            "Train Epoch: 1420 \tLoss: 0.277668\n",
            "Train Epoch: 1425 \tLoss: 0.276585\n",
            "\n",
            "Test set: Avg. loss: 0.4402, Accuracy: 432/500 (86%)\n",
            "\n",
            "Train Epoch: 1430 \tLoss: 0.275509\n",
            "Train Epoch: 1435 \tLoss: 0.274439\n",
            "Train Epoch: 1440 \tLoss: 0.273376\n",
            "Train Epoch: 1445 \tLoss: 0.272319\n",
            "Train Epoch: 1450 \tLoss: 0.271268\n",
            "\n",
            "Test set: Avg. loss: 0.4351, Accuracy: 432/500 (86%)\n",
            "\n",
            "Train Epoch: 1455 \tLoss: 0.270223\n",
            "Train Epoch: 1460 \tLoss: 0.269185\n",
            "Train Epoch: 1465 \tLoss: 0.268153\n",
            "Train Epoch: 1470 \tLoss: 0.267127\n",
            "Train Epoch: 1475 \tLoss: 0.266107\n",
            "\n",
            "Test set: Avg. loss: 0.4302, Accuracy: 432/500 (86%)\n",
            "\n",
            "Train Epoch: 1480 \tLoss: 0.265092\n",
            "Train Epoch: 1485 \tLoss: 0.264084\n",
            "Train Epoch: 1490 \tLoss: 0.263082\n",
            "Train Epoch: 1495 \tLoss: 0.262085\n",
            "Train Epoch: 1500 \tLoss: 0.261095\n",
            "\n",
            "Test set: Avg. loss: 0.4254, Accuracy: 432/500 (86%)\n",
            "\n",
            "Train Epoch: 1505 \tLoss: 0.260112\n",
            "Train Epoch: 1510 \tLoss: 0.259134\n",
            "Train Epoch: 1515 \tLoss: 0.258162\n",
            "Train Epoch: 1520 \tLoss: 0.257197\n",
            "Train Epoch: 1525 \tLoss: 0.256237\n",
            "\n",
            "Test set: Avg. loss: 0.4207, Accuracy: 432/500 (86%)\n",
            "\n",
            "Train Epoch: 1530 \tLoss: 0.255282\n",
            "Train Epoch: 1535 \tLoss: 0.254333\n",
            "Train Epoch: 1540 \tLoss: 0.253388\n",
            "Train Epoch: 1545 \tLoss: 0.252449\n",
            "Train Epoch: 1550 \tLoss: 0.251516\n",
            "\n",
            "Test set: Avg. loss: 0.4162, Accuracy: 433/500 (87%)\n",
            "\n",
            "Train Epoch: 1555 \tLoss: 0.250588\n",
            "Train Epoch: 1560 \tLoss: 0.249665\n",
            "Train Epoch: 1565 \tLoss: 0.248747\n",
            "Train Epoch: 1570 \tLoss: 0.247834\n",
            "Train Epoch: 1575 \tLoss: 0.246926\n",
            "\n",
            "Test set: Avg. loss: 0.4118, Accuracy: 433/500 (87%)\n",
            "\n",
            "Train Epoch: 1580 \tLoss: 0.246022\n",
            "Train Epoch: 1585 \tLoss: 0.245122\n",
            "Train Epoch: 1590 \tLoss: 0.244228\n",
            "Train Epoch: 1595 \tLoss: 0.243339\n",
            "Train Epoch: 1600 \tLoss: 0.242454\n",
            "\n",
            "Test set: Avg. loss: 0.4075, Accuracy: 435/500 (87%)\n",
            "\n",
            "Train Epoch: 1605 \tLoss: 0.241575\n",
            "Train Epoch: 1610 \tLoss: 0.240700\n",
            "Train Epoch: 1615 \tLoss: 0.239830\n",
            "Train Epoch: 1620 \tLoss: 0.238965\n",
            "Train Epoch: 1625 \tLoss: 0.238105\n",
            "\n",
            "Test set: Avg. loss: 0.4033, Accuracy: 435/500 (87%)\n",
            "\n",
            "Train Epoch: 1630 \tLoss: 0.237251\n",
            "Train Epoch: 1635 \tLoss: 0.236401\n",
            "Train Epoch: 1640 \tLoss: 0.235557\n",
            "Train Epoch: 1645 \tLoss: 0.234718\n",
            "Train Epoch: 1650 \tLoss: 0.233883\n",
            "\n",
            "Test set: Avg. loss: 0.3993, Accuracy: 435/500 (87%)\n",
            "\n",
            "Train Epoch: 1655 \tLoss: 0.233053\n",
            "Train Epoch: 1660 \tLoss: 0.232228\n",
            "Train Epoch: 1665 \tLoss: 0.231408\n",
            "Train Epoch: 1670 \tLoss: 0.230592\n",
            "Train Epoch: 1675 \tLoss: 0.229781\n",
            "\n",
            "Test set: Avg. loss: 0.3953, Accuracy: 434/500 (87%)\n",
            "\n",
            "Train Epoch: 1680 \tLoss: 0.228974\n",
            "Train Epoch: 1685 \tLoss: 0.228171\n",
            "Train Epoch: 1690 \tLoss: 0.227372\n",
            "Train Epoch: 1695 \tLoss: 0.226577\n",
            "Train Epoch: 1700 \tLoss: 0.225787\n",
            "\n",
            "Test set: Avg. loss: 0.3915, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 1705 \tLoss: 0.225000\n",
            "Train Epoch: 1710 \tLoss: 0.224217\n",
            "Train Epoch: 1715 \tLoss: 0.223439\n",
            "Train Epoch: 1720 \tLoss: 0.222665\n",
            "Train Epoch: 1725 \tLoss: 0.221894\n",
            "\n",
            "Test set: Avg. loss: 0.3878, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 1730 \tLoss: 0.221127\n",
            "Train Epoch: 1735 \tLoss: 0.220365\n",
            "Train Epoch: 1740 \tLoss: 0.219606\n",
            "Train Epoch: 1745 \tLoss: 0.218851\n",
            "Train Epoch: 1750 \tLoss: 0.218101\n",
            "\n",
            "Test set: Avg. loss: 0.3841, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 1755 \tLoss: 0.217354\n",
            "Train Epoch: 1760 \tLoss: 0.216612\n",
            "Train Epoch: 1765 \tLoss: 0.215872\n",
            "Train Epoch: 1770 \tLoss: 0.215137\n",
            "Train Epoch: 1775 \tLoss: 0.214404\n",
            "\n",
            "Test set: Avg. loss: 0.3806, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 1780 \tLoss: 0.213675\n",
            "Train Epoch: 1785 \tLoss: 0.212950\n",
            "Train Epoch: 1790 \tLoss: 0.212229\n",
            "Train Epoch: 1795 \tLoss: 0.211512\n",
            "Train Epoch: 1800 \tLoss: 0.210798\n",
            "\n",
            "Test set: Avg. loss: 0.3771, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 1805 \tLoss: 0.210088\n",
            "Train Epoch: 1810 \tLoss: 0.209381\n",
            "Train Epoch: 1815 \tLoss: 0.208678\n",
            "Train Epoch: 1820 \tLoss: 0.207979\n",
            "Train Epoch: 1825 \tLoss: 0.207283\n",
            "\n",
            "Test set: Avg. loss: 0.3738, Accuracy: 436/500 (87%)\n",
            "\n",
            "Train Epoch: 1830 \tLoss: 0.206590\n",
            "Train Epoch: 1835 \tLoss: 0.205901\n",
            "Train Epoch: 1840 \tLoss: 0.205215\n",
            "Train Epoch: 1845 \tLoss: 0.204532\n",
            "Train Epoch: 1850 \tLoss: 0.203853\n",
            "\n",
            "Test set: Avg. loss: 0.3705, Accuracy: 439/500 (88%)\n",
            "\n",
            "Train Epoch: 1855 \tLoss: 0.203176\n",
            "Train Epoch: 1860 \tLoss: 0.202503\n",
            "Train Epoch: 1865 \tLoss: 0.201834\n",
            "Train Epoch: 1870 \tLoss: 0.201167\n",
            "Train Epoch: 1875 \tLoss: 0.200504\n",
            "\n",
            "Test set: Avg. loss: 0.3673, Accuracy: 440/500 (88%)\n",
            "\n",
            "Train Epoch: 1880 \tLoss: 0.199844\n",
            "Train Epoch: 1885 \tLoss: 0.199188\n",
            "Train Epoch: 1890 \tLoss: 0.198534\n",
            "Train Epoch: 1895 \tLoss: 0.197885\n",
            "Train Epoch: 1900 \tLoss: 0.197238\n",
            "\n",
            "Test set: Avg. loss: 0.3642, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 1905 \tLoss: 0.196595\n",
            "Train Epoch: 1910 \tLoss: 0.195956\n",
            "Train Epoch: 1915 \tLoss: 0.195319\n",
            "Train Epoch: 1920 \tLoss: 0.194686\n",
            "Train Epoch: 1925 \tLoss: 0.194055\n",
            "\n",
            "Test set: Avg. loss: 0.3612, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 1930 \tLoss: 0.193428\n",
            "Train Epoch: 1935 \tLoss: 0.192804\n",
            "Train Epoch: 1940 \tLoss: 0.192182\n",
            "Train Epoch: 1945 \tLoss: 0.191563\n",
            "Train Epoch: 1950 \tLoss: 0.190947\n",
            "\n",
            "Test set: Avg. loss: 0.3582, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 1955 \tLoss: 0.190333\n",
            "Train Epoch: 1960 \tLoss: 0.189723\n",
            "Train Epoch: 1965 \tLoss: 0.189115\n",
            "Train Epoch: 1970 \tLoss: 0.188511\n",
            "Train Epoch: 1975 \tLoss: 0.187909\n",
            "\n",
            "Test set: Avg. loss: 0.3554, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 1980 \tLoss: 0.187310\n",
            "Train Epoch: 1985 \tLoss: 0.186714\n",
            "Train Epoch: 1990 \tLoss: 0.186121\n",
            "Train Epoch: 1995 \tLoss: 0.185532\n",
            "Train Epoch: 2000 \tLoss: 0.184945\n",
            "\n",
            "Test set: Avg. loss: 0.3526, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 2005 \tLoss: 0.184360\n",
            "Train Epoch: 2010 \tLoss: 0.183779\n",
            "Train Epoch: 2015 \tLoss: 0.183201\n",
            "Train Epoch: 2020 \tLoss: 0.182625\n",
            "Train Epoch: 2025 \tLoss: 0.182051\n",
            "\n",
            "Test set: Avg. loss: 0.3499, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 2030 \tLoss: 0.181479\n",
            "Train Epoch: 2035 \tLoss: 0.180911\n",
            "Train Epoch: 2040 \tLoss: 0.180345\n",
            "Train Epoch: 2045 \tLoss: 0.179781\n",
            "Train Epoch: 2050 \tLoss: 0.179220\n",
            "\n",
            "Test set: Avg. loss: 0.3472, Accuracy: 441/500 (88%)\n",
            "\n",
            "Train Epoch: 2055 \tLoss: 0.178661\n",
            "Train Epoch: 2060 \tLoss: 0.178105\n",
            "Train Epoch: 2065 \tLoss: 0.177551\n",
            "Train Epoch: 2070 \tLoss: 0.177000\n",
            "Train Epoch: 2075 \tLoss: 0.176451\n",
            "\n",
            "Test set: Avg. loss: 0.3446, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2080 \tLoss: 0.175905\n",
            "Train Epoch: 2085 \tLoss: 0.175361\n",
            "Train Epoch: 2090 \tLoss: 0.174819\n",
            "Train Epoch: 2095 \tLoss: 0.174279\n",
            "Train Epoch: 2100 \tLoss: 0.173742\n",
            "\n",
            "Test set: Avg. loss: 0.3421, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2105 \tLoss: 0.173207\n",
            "Train Epoch: 2110 \tLoss: 0.172674\n",
            "Train Epoch: 2115 \tLoss: 0.172144\n",
            "Train Epoch: 2120 \tLoss: 0.171617\n",
            "Train Epoch: 2125 \tLoss: 0.171092\n",
            "\n",
            "Test set: Avg. loss: 0.3396, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2130 \tLoss: 0.170569\n",
            "Train Epoch: 2135 \tLoss: 0.170047\n",
            "Train Epoch: 2140 \tLoss: 0.169528\n",
            "Train Epoch: 2145 \tLoss: 0.169012\n",
            "Train Epoch: 2150 \tLoss: 0.168497\n",
            "\n",
            "Test set: Avg. loss: 0.3372, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2155 \tLoss: 0.167985\n",
            "Train Epoch: 2160 \tLoss: 0.167475\n",
            "Train Epoch: 2165 \tLoss: 0.166967\n",
            "Train Epoch: 2170 \tLoss: 0.166461\n",
            "Train Epoch: 2175 \tLoss: 0.165957\n",
            "\n",
            "Test set: Avg. loss: 0.3348, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2180 \tLoss: 0.165455\n",
            "Train Epoch: 2185 \tLoss: 0.164955\n",
            "Train Epoch: 2190 \tLoss: 0.164457\n",
            "Train Epoch: 2195 \tLoss: 0.163961\n",
            "Train Epoch: 2200 \tLoss: 0.163467\n",
            "\n",
            "Test set: Avg. loss: 0.3325, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2205 \tLoss: 0.162975\n",
            "Train Epoch: 2210 \tLoss: 0.162485\n",
            "Train Epoch: 2215 \tLoss: 0.161997\n",
            "Train Epoch: 2220 \tLoss: 0.161511\n",
            "Train Epoch: 2225 \tLoss: 0.161026\n",
            "\n",
            "Test set: Avg. loss: 0.3303, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2230 \tLoss: 0.160544\n",
            "Train Epoch: 2235 \tLoss: 0.160064\n",
            "Train Epoch: 2240 \tLoss: 0.159585\n",
            "Train Epoch: 2245 \tLoss: 0.159108\n",
            "Train Epoch: 2250 \tLoss: 0.158633\n",
            "\n",
            "Test set: Avg. loss: 0.3281, Accuracy: 442/500 (88%)\n",
            "\n",
            "Train Epoch: 2255 \tLoss: 0.158161\n",
            "Train Epoch: 2260 \tLoss: 0.157690\n",
            "Train Epoch: 2265 \tLoss: 0.157222\n",
            "Train Epoch: 2270 \tLoss: 0.156755\n",
            "Train Epoch: 2275 \tLoss: 0.156290\n",
            "\n",
            "Test set: Avg. loss: 0.3259, Accuracy: 443/500 (89%)\n",
            "\n",
            "Train Epoch: 2280 \tLoss: 0.155828\n",
            "Train Epoch: 2285 \tLoss: 0.155367\n",
            "Train Epoch: 2290 \tLoss: 0.154908\n",
            "Train Epoch: 2295 \tLoss: 0.154450\n",
            "Train Epoch: 2300 \tLoss: 0.153994\n",
            "\n",
            "Test set: Avg. loss: 0.3238, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2305 \tLoss: 0.153540\n",
            "Train Epoch: 2310 \tLoss: 0.153088\n",
            "Train Epoch: 2315 \tLoss: 0.152638\n",
            "Train Epoch: 2320 \tLoss: 0.152190\n",
            "Train Epoch: 2325 \tLoss: 0.151743\n",
            "\n",
            "Test set: Avg. loss: 0.3217, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2330 \tLoss: 0.151298\n",
            "Train Epoch: 2335 \tLoss: 0.150855\n",
            "Train Epoch: 2340 \tLoss: 0.150414\n",
            "Train Epoch: 2345 \tLoss: 0.149975\n",
            "Train Epoch: 2350 \tLoss: 0.149537\n",
            "\n",
            "Test set: Avg. loss: 0.3197, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2355 \tLoss: 0.149101\n",
            "Train Epoch: 2360 \tLoss: 0.148667\n",
            "Train Epoch: 2365 \tLoss: 0.148235\n",
            "Train Epoch: 2370 \tLoss: 0.147805\n",
            "Train Epoch: 2375 \tLoss: 0.147376\n",
            "\n",
            "Test set: Avg. loss: 0.3177, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2380 \tLoss: 0.146949\n",
            "Train Epoch: 2385 \tLoss: 0.146524\n",
            "Train Epoch: 2390 \tLoss: 0.146101\n",
            "Train Epoch: 2395 \tLoss: 0.145679\n",
            "Train Epoch: 2400 \tLoss: 0.145259\n",
            "\n",
            "Test set: Avg. loss: 0.3158, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2405 \tLoss: 0.144841\n",
            "Train Epoch: 2410 \tLoss: 0.144424\n",
            "Train Epoch: 2415 \tLoss: 0.144009\n",
            "Train Epoch: 2420 \tLoss: 0.143595\n",
            "Train Epoch: 2425 \tLoss: 0.143183\n",
            "\n",
            "Test set: Avg. loss: 0.3139, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2430 \tLoss: 0.142773\n",
            "Train Epoch: 2435 \tLoss: 0.142364\n",
            "Train Epoch: 2440 \tLoss: 0.141957\n",
            "Train Epoch: 2445 \tLoss: 0.141551\n",
            "Train Epoch: 2450 \tLoss: 0.141147\n",
            "\n",
            "Test set: Avg. loss: 0.3121, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2455 \tLoss: 0.140744\n",
            "Train Epoch: 2460 \tLoss: 0.140343\n",
            "Train Epoch: 2465 \tLoss: 0.139944\n",
            "Train Epoch: 2470 \tLoss: 0.139546\n",
            "Train Epoch: 2475 \tLoss: 0.139149\n",
            "\n",
            "Test set: Avg. loss: 0.3102, Accuracy: 444/500 (89%)\n",
            "\n",
            "Train Epoch: 2480 \tLoss: 0.138754\n",
            "Train Epoch: 2485 \tLoss: 0.138361\n",
            "Train Epoch: 2490 \tLoss: 0.137969\n",
            "Train Epoch: 2495 \tLoss: 0.137578\n",
            "Train Epoch: 2500 \tLoss: 0.137190\n",
            "\n",
            "Test set: Avg. loss: 0.3085, Accuracy: 445/500 (89%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_n = 25\n",
        "test()\n",
        "count = []\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  if epoch % test_n == 0:\n",
        "    test()\n",
        "    count.append(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.arange(0,n_epochs)\n",
        "count = np.arange(0,n_epochs+test_n,test_n)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(x, train_losses, color='blue', zorder=1)\n",
        "plt.plot(count, test_losses, color='red', zorder=2)\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    network.fc1.weight[:] = fc1_init\n",
        "    #network.fc2.weight[:] = fc2_init\n",
        "    #network.conv1.weight[:] = conv1_init\n",
        "    #network.conv2.weight[:] = conv2_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor(2.2958),\n",
              " tensor(2.2528),\n",
              " tensor(2.2058),\n",
              " tensor(2.1492),\n",
              " tensor(2.0791),\n",
              " tensor(1.9939),\n",
              " tensor(1.8955),\n",
              " tensor(1.7894),\n",
              " tensor(1.6817),\n",
              " tensor(1.5770),\n",
              " tensor(1.4784),\n",
              " tensor(1.3874),\n",
              " tensor(1.3048),\n",
              " tensor(1.2305),\n",
              " tensor(1.1643),\n",
              " tensor(1.1054),\n",
              " tensor(1.0532),\n",
              " tensor(1.0068),\n",
              " tensor(0.9653),\n",
              " tensor(0.9281),\n",
              " tensor(0.8946),\n",
              " tensor(0.8641),\n",
              " tensor(0.8363),\n",
              " tensor(0.8108),\n",
              " tensor(0.7873),\n",
              " tensor(0.7655),\n",
              " tensor(0.7452),\n",
              " tensor(0.7263),\n",
              " tensor(0.7085),\n",
              " tensor(0.6919),\n",
              " tensor(0.6762),\n",
              " tensor(0.6614),\n",
              " tensor(0.6475),\n",
              " tensor(0.6342),\n",
              " tensor(0.6216),\n",
              " tensor(0.6096),\n",
              " tensor(0.5982),\n",
              " tensor(0.5873),\n",
              " tensor(0.5769),\n",
              " tensor(0.5670),\n",
              " tensor(0.5575),\n",
              " tensor(0.5483),\n",
              " tensor(0.5396),\n",
              " tensor(0.5311),\n",
              " tensor(0.5231),\n",
              " tensor(0.5153),\n",
              " tensor(0.5078),\n",
              " tensor(0.5006),\n",
              " tensor(0.4936),\n",
              " tensor(0.4869),\n",
              " tensor(0.4803),\n",
              " tensor(0.4740),\n",
              " tensor(0.4679),\n",
              " tensor(0.4621),\n",
              " tensor(0.4563),\n",
              " tensor(0.4508),\n",
              " tensor(0.4454),\n",
              " tensor(0.4402),\n",
              " tensor(0.4351),\n",
              " tensor(0.4302),\n",
              " tensor(0.4254),\n",
              " tensor(0.4207),\n",
              " tensor(0.4162),\n",
              " tensor(0.4118),\n",
              " tensor(0.4075),\n",
              " tensor(0.4033),\n",
              " tensor(0.3993),\n",
              " tensor(0.3953),\n",
              " tensor(0.3915),\n",
              " tensor(0.3878),\n",
              " tensor(0.3841),\n",
              " tensor(0.3806),\n",
              " tensor(0.3771),\n",
              " tensor(0.3738),\n",
              " tensor(0.3705),\n",
              " tensor(0.3673),\n",
              " tensor(0.3642),\n",
              " tensor(0.3612),\n",
              " tensor(0.3582),\n",
              " tensor(0.3554),\n",
              " tensor(0.3526),\n",
              " tensor(0.3499),\n",
              " tensor(0.3472),\n",
              " tensor(0.3446),\n",
              " tensor(0.3421),\n",
              " tensor(0.3396),\n",
              " tensor(0.3372),\n",
              " tensor(0.3348),\n",
              " tensor(0.3325),\n",
              " tensor(0.3303),\n",
              " tensor(0.3281),\n",
              " tensor(0.3259),\n",
              " tensor(0.3238),\n",
              " tensor(0.3217),\n",
              " tensor(0.3197),\n",
              " tensor(0.3177),\n",
              " tensor(0.3158),\n",
              " tensor(0.3139),\n",
              " tensor(0.3121),\n",
              " tensor(0.3102),\n",
              " tensor(0.3085)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_losses"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
